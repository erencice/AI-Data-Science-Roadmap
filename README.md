# AI & Data Science Weekly Plan â€” Activities, Practice, and Pass Criteria

![Duration](https://img.shields.io/badge/duration-~209_weeks-6f42c1)
![Pace](https://img.shields.io/badge/pace-8â€“10_h%2Fweek-0e8a16)
![Path](https://img.shields.io/badge/path-beginner%E2%86%92practitioner-0366d6)
![Style](https://img.shields.io/badge/style-cumulative%2C_concept%E2%86%92practice-555)

Zero prior knowledge is assumed. Learning order is strictly top-to-bottom. Each week includes a clear â€œPassâ€ requirement aligned to the primary resource.

â€” Quick jump â€”
- Phase 1 Â· Data Analysis Foundations
- Phase 14 Â· Web Scraping & SQL
- Phase 2 Â· Mathematics for ML
- Phase 11 Â· Convex Optimization
- Phase 3 Â· Statistics Fundamentals
- Phase 4 Â· Applied Multivariate Statistics
- Phase 5 Â· Bayesian Statistics & Missing Data
- Phase 6 Â· Statistical Learning with Python (ISLP)
- Phase 8 Â· Data Mining
- Phase 7 Â· Classical ML
- Phase 12 Â· Elements of Statistical Learning
- Phase 15 Â· Deep Learning
- Phase 17 Â· LLMs & Open-Source AI
- Phase 13 Â· R for Data Science
- Phase 9 Â· Econometrics & Time Series
- Phase 10 Â· Causal Inference
- Phase 16 Â· MLOps & Data Engineering
- Phase 18 Â· Consolidation & Capstone

Legend
- ğŸ“– Activities (primary source)
- ğŸ§ª Practice (small tasks)
- âœ… Pass (weekly pass criterion)
- ğŸ› ï¸ How (implementation hint)
- ğŸ” Flex (catch-up, spaced review)

Duration and pacing
- Duration: ~209 weeks (â‰ˆ4.0 years), 8â€“10 h/week
- Weekly output: small practical tasks only
- Frequent Flex Weeks between phases for consolidation

Main resources (cover-to-cover completion)
- Python for Data Analysis â€” Wes McKinney â€” [Python for Data Analysis](https://wesmckinney.com/book/)
- Mathematics for Machine Learning â€” Deisenroth, Faisal, Ong â€” [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- Think Stats â€” Allen B. Downey â€” [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- Think Bayes â€” Allen B. Downey â€” [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- Flexible Imputation of Missing Data â€” van Buuren â€” [FIMD](https://stefvanbuuren.name/fimd/)
- An Introduction to Statistical Learning with Applications in Python â€” James, Witten, Hastie, Tibshirani â€” [ISLP](https://www.statlearning.com/)
- Pattern Recognition and Machine Learning â€” Bishop â€” [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- Interpretable Machine Learning â€” Molnar â€” [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- Data Mining: Concepts and Techniques (3e) â€” Han, Kamber, Pei â€” [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- Basic Econometrics â€” Gujarati â€” [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- New Introduction to Multiple Time Series â€” LÃ¼tkepohl â€” [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- Causal Inference: The Mixtape â€” Cunningham â€” [The Mixtape](https://mixtape.scunning.com)
- Convex Optimization â€” Boyd & Vandenberghe â€” [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- The Elements of Statistical Learning â€” Hastie, Tibshirani, Friedman â€” [ESL](https://hastie.su.domains/ElemStatLearn/)
- R for Data Science (2e) â€” Wickham, Ã‡etinkaya-Rundel, Grolemund â€” [R for Data Science (2e)](https://r4ds.hadley.nz)
- Beautiful Soup docs â€” [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- Selenium (Python) docs â€” [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- SQL Roadmap â€” [SQL Roadmap (GeeksforGeeks)](https://www.geeksforgeeks.org/blogs/sql-roadmap/)
- Dive into Deep Learning â€” Zhang et al. â€” [D2L](https://d2l.ai)
- Deep Learning â€” Goodfellow, Bengio, Courville â€” [Deep Learning Book](https://www.deeplearningbook.org/)
- MLOps Zoomcamp â€” DataTalksClub â€” [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- Machine Learning Systems â€” Symeonidis et al. â€” [ML Systems](https://mlsysbook.ai)
- Data Engineering Zoomcamp â€” DataTalksClub â€” [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- Hugging Face Course â€” [HF Course](https://huggingface.co/course/chapter1)
- HF Agents Course â€” [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)

Supporting references (selective)
- Trigonometric Cheat Sheet â€” [Trig Sheet (PDF)](https://tutorial.math.lamar.edu/pdf/Trig_Cheat_Sheet.pdf)
- Python Crash Course â€” [Video](https://www.youtube.com/watch?v=rfscVS0vtbw)
- Kevin Sheppard Python Notes â€” [Notes (PDF)](https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2021.pdf)
- PSU STAT â€” [STAT portal](https://online.stat.psu.edu)
- scikit-learn docs â€” [scikit-learn](https://scikit-learn.org/stable/index.html)
- statsmodels docs â€” [statsmodels](https://www.statsmodels.org/stable/index.html)

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 1 Â· Data Analysis Foundations â€” Weeks 1â€“8 (Complete Python for Data Analysis)</b></summary>

Week 1 â€” P4DA Ch. 1â€“2
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Set up Python environment (conda/pip); run IPython/Jupyter; practice Python basics (variables, control flow, functions); understand the data analysis ecosystem.
- âœ… Pass: Create a notebook demonstrating Python fundamentals: define 3 functions, use list/dict comprehensions, write a simple script that reads command-line arguments, and explain the role of NumPy/pandas/matplotlib in the data stack.
- ğŸ› ï¸ How: Install Anaconda or miniconda; launch Jupyter; experiment with built-in types and control structures; skim the library overview in Ch.1.

Week 2 â€” P4DA Ch. 3â€“4
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Work with tuples, lists, dicts, sets (Ch.3); create and manipulate NumPy ndarrays; practice array indexing, slicing, and vectorized operations (Ch.4).
- âœ… Pass: Build a notebook that: (1) demonstrates list/dict/set operations; (2) creates 2D NumPy arrays, performs element-wise and matrix operations; (3) uses boolean indexing to filter data; (4) times vectorized vs loop-based computation.
- ğŸ› ï¸ How: `np.array`, `np.arange`, `np.reshape`, boolean masks, `np.where`, `%timeit` to compare performance.

Week 3 â€” P4DA Ch. 5â€“6
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Create Series and DataFrames; use `.loc/.iloc` indexing; load data from CSV/JSON/Excel files (Ch.5â€“6).
- âœ… Pass: Load a dataset from CSV, inspect with `.head()/.info()/.describe()`, select columns via `.loc/.iloc`, filter rows with boolean masks, and export cleaned data to a new CSV.
- ğŸ› ï¸ How: `pd.read_csv`, `pd.read_json`, `df.loc[rows, cols]`, `df.iloc[row_idx, col_idx]`, `df.to_csv`.

Week 4 â€” P4DA Ch. 7â€“8
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Handle missing data; clean strings with `.str` methods; merge/join DataFrames; reshape with `stack/unstack/pivot/melt` (Ch.7â€“8).
- âœ… Pass: Take a messy dataset and: (1) handle missing values (drop or fill); (2) standardize string columns (trim/lower); (3) merge with a second table; (4) pivot or melt the result; document row counts at each step.
- ğŸ› ï¸ How: `df.dropna`, `df.fillna`, `df["col"].str.strip().str.lower()`, `pd.merge`, `pd.pivot_table`, `pd.melt`.

Week 5 â€” P4DA Ch. 9â€“10
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Create plots with matplotlib/seaborn (Ch.9); perform aggregation with `groupby` (Ch.10).
- âœ… Pass: Produce 4 visualizations (histogram, scatter, line, bar) with proper labels/titles; use `groupby().agg()` to compute multi-column summaries; combine groupby results with plots.
- ğŸ› ï¸ How: `plt.plot`, `plt.hist`, `sns.scatterplot`, `df.groupby("col").agg({"num":"mean"})`, `plt.savefig`.

Week 6 â€” P4DA Ch. 11â€“12 (+appendices)
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Work with time series: DateTimeIndex, resampling, rolling windows (Ch.11); explore advanced pandas: Categoricals, method chaining, performance (Ch.12).
- âœ… Pass: Load time series data, set DateTimeIndex, resample to weekly/monthly, compute rolling statistics; convert a column to Categorical; refactor pipeline using method chaining; time vectorized vs apply.
- ğŸ› ï¸ How: `pd.to_datetime`, `df.set_index`, `df.resample("W").mean()`, `.rolling(7).mean()`, `pd.Categorical`, `.pipe()`.

Week 7 â€” P4DA Project A
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: End-to-end EDA pipeline using all chapters 1â€“12: load, clean, transform, aggregate, visualize.
- âœ… Pass: Apply a complete EDA workflow to a new dataset; produce â‰¥5 visualizations; write a 1-page summary with â‰¥3 insights, â‰¥2 hypotheses, and â‰¥1 data quality issue identified.
- ğŸ› ï¸ How: Combine prior weeks' functions into a reusable pipeline; keep code modular and well-documented.

Week 8 â€” P4DA Project B
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Feature engineering using transforms from the book: date/time features, categorical encoding, ratios, binning.
- âœ… Pass: Create â‰¥5 derived features (date parts, ratios, binned numerics, category combinations); document each feature's rationale, potential predictive value, and leakage risk.
- ğŸ› ï¸ How: `df["date"].dt.month`, `df.assign(ratio=lambda x: x["a"]/x["b"])`, `pd.cut`, `pd.get_dummies`.
</details>

ğŸ” Flex â€” Consolidate EDA template and notes

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 14 Â· Web Scraping & SQL â€” Weeks 152â€“157 (Complete BeautifulSoup, Selenium, SQL)</b></summary>

Week 152 â€” BeautifulSoup
- ğŸ“– [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- ğŸ§ª Practice: Scrape static HTML pages: fetch with requests, parse with BeautifulSoup, navigate the DOM, extract data using CSS selectors and tag methods.
- âœ… Pass: Scrape a static website and extract structured data; save as CSV/JSON with documented schema; check robots.txt before scraping; implement polite delays to avoid rate limiting (no HTTP 429 errors).
- ğŸ› ï¸ How: `requests.get(url)`; `BeautifulSoup(html, "lxml")`; `.select()` for CSS selectors; `.find_all()` for tag-based search; `time.sleep()` between requests.

Weeks 153â€“154 â€” Selenium
- ğŸ“– [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- ğŸ§ª Practice: Automate browser interactions for dynamic websites: handle JavaScript-rendered content, implement explicit waits, manage pagination and infinite scroll, fill forms.
- âœ… Pass (weekly): Scrape a JavaScript-rendered page (e.g., infinite scroll or content behind clicks); implement proper waits and error handling; save timestamped data with retry/timeout logs; handle at least one failure scenario gracefully.
- ğŸ› ï¸ How: `webdriver.Chrome()`; `WebDriverWait` with `expected_conditions`; CSS/XPath selectors; `execute_script()` for scrolling; consistent viewport settings.

Week 155 â€” SQL Basics & Data Retrieval
- ğŸ“– [SQL Roadmap (GeeksforGeeks)](https://www.geeksforgeeks.org/blogs/sql-roadmap/)
- ğŸ§ª Practice: Set up SQL environment (SQLite/PostgreSQL/MySQL); understand database structure (tables, rows, columns); master SELECT statement; use WHERE clause with comparison operators (=, !=, <, >, <=, >=); apply logical operators (AND, OR, NOT); sort results with ORDER BY; limit results with LIMIT/TOP; use DISTINCT to remove duplicates; practice basic string matching with LIKE and wildcards (%, _).
- âœ… Pass: Install and configure a SQL database system; create a sample database with at least 3 tables; write â‰¥20 SELECT queries demonstrating: simple selection, filtering with WHERE, multiple conditions with AND/OR, sorting ascending/descending, limiting results, removing duplicates, and pattern matching with LIKE; document each query with its purpose and expected row count.
- ğŸ› ï¸ How: Install SQLite (lightweight) or PostgreSQL (production-grade); use `CREATE TABLE`, `INSERT INTO` for sample data; practice `SELECT * FROM table`, `SELECT col1, col2 FROM table WHERE condition`, `ORDER BY col ASC/DESC`, `LIMIT n`, `DISTINCT col`, `WHERE col LIKE 'pattern%'`; use a SQL client (DBeaver, pgAdmin, or command line).

Week 156 â€” SQL Joins, Aggregations & Subqueries
- ğŸ“– [SQL Roadmap (GeeksforGeeks)](https://www.geeksforgeeks.org/blogs/sql-roadmap/)
- ğŸ§ª Practice: Master different types of joins (INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL OUTER JOIN, CROSS JOIN, SELF JOIN); understand when to use each join type; work with aggregate functions (COUNT, SUM, AVG, MIN, MAX); use GROUP BY for grouping data; filter grouped data with HAVING clause; write subqueries in SELECT, WHERE, and FROM clauses; understand correlated vs non-correlated subqueries; practice set operations (UNION, UNION ALL, INTERSECT, EXCEPT); work with NULL values (IS NULL, IS NOT NULL, COALESCE, NULLIF).
- âœ… Pass: Create a relational schema with â‰¥3 related tables (e.g., customers, orders, products); write â‰¥25 queries demonstrating: all join types with explanations, aggregate functions with and without GROUP BY, HAVING clause filters, nested subqueries (at least 2 levels deep), correlated subqueries, set operations combining multiple queries, NULL handling in various contexts; include at least one complex multi-join query involving 3+ tables; document the business question each query answers.
- ğŸ› ï¸ How: Design schema with foreign key relationships; `INNER JOIN table2 ON table1.id = table2.fk_id`, `LEFT JOIN` for including unmatched rows; `SELECT COUNT(*), AVG(col) FROM table GROUP BY category`, `HAVING COUNT(*) > 5`; `WHERE col IN (SELECT...)`, `SELECT (SELECT...) AS subquery_col`; `UNION` to combine results; `COALESCE(col, 'default')` for NULL handling; visualize join results with Venn diagrams.

Week 157 â€” Advanced SQL: Window Functions, CTEs & Performance
- ğŸ“– [SQL Roadmap (GeeksforGeeks)](https://www.geeksforgeeks.org/blogs/sql-roadmap/)
- ğŸ§ª Practice: Master window functions (ROW_NUMBER, RANK, DENSE_RANK, NTILE); use aggregate window functions (SUM, AVG, COUNT over partitions); apply LEAD and LAG for accessing adjacent rows; work with PARTITION BY and ORDER BY in window functions; write Common Table Expressions (CTEs) for readable complex queries; use recursive CTEs for hierarchical data; understand query execution plans and optimization; create and use indexes for performance; practice transactions (BEGIN, COMMIT, ROLLBACK); work with views and stored procedures; implement data manipulation (INSERT, UPDATE, DELETE) with complex conditions; understand constraints (PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK); explore date/time functions and string manipulation functions.
- âœ… Pass: Build a complete analytics database schema with â‰¥4 tables and proper constraints; write â‰¥30 advanced queries including: â‰¥10 window function queries (ranking, running totals, moving averages, lag/lead analysis), â‰¥5 CTEs (including at least 1 recursive CTE for hierarchical data like org charts or category trees), â‰¥5 queries with EXPLAIN/ANALYZE showing index usage, â‰¥3 transactions demonstrating ACID properties, â‰¥3 views encapsulating complex logic, â‰¥5 DML operations (INSERT/UPDATE/DELETE with subqueries), date calculations (date differences, date parts, date formatting), and complex string manipulations; create indexes and demonstrate query performance improvement; document each query with execution time before/after optimization where applicable.
- ğŸ› ï¸ How: Window functions: `ROW_NUMBER() OVER (PARTITION BY col ORDER BY col2)`, `SUM(col) OVER (PARTITION BY category ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)` for running totals; CTEs: `WITH cte_name AS (SELECT...) SELECT * FROM cte_name`; Recursive CTE: `WITH RECURSIVE cte AS (base_case UNION ALL recursive_case) SELECT * FROM cte`; Indexes: `CREATE INDEX idx_name ON table(col)`; check execution plan with `EXPLAIN` (PostgreSQL) or `EXPLAIN QUERY PLAN` (SQLite); transactions: `BEGIN; UPDATE...; COMMIT;`; views: `CREATE VIEW view_name AS SELECT...`; use `EXTRACT`, `DATE_TRUNC`, `AGE` for dates; `CONCAT`, `SUBSTRING`, `REGEXP_REPLACE` for strings; optimize with proper WHERE clause ordering and index hints.

</details>

ğŸ” Flex â€” ETL mini-project

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 2 Â· Mathematics for ML â€” Weeks 9â€“18 (Complete MML)</b></summary>

Week 9 â€” Linear Algebra I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Vectors (addition, scalar multiplication, norms); matrix operations (transpose, multiplication); linear independence and basis.
- âœ… Pass: Implement vector/matrix operations from scratch; verify linear independence of a set of vectors; compute and interpret different vector norms (L1, L2, Linf).
- ğŸ› ï¸ How: `np.dot`, `np.linalg.norm`, `np.linalg.matrix_rank`; manually verify independence via row reduction.

Week 10 â€” Linear Algebra II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Eigenvalues and eigenvectors; matrix diagonalization; positive definiteness; condition number.
- âœ… Pass: Compute eigendecomposition of symmetric matrices; verify diagonalization A = PDPâ»Â¹; check positive definiteness via eigenvalues; interpret condition number for numerical stability.
- ğŸ› ï¸ How: `np.linalg.eig`, `np.linalg.eigh` for symmetric; `np.linalg.cond`; verify reconstruction.

Week 11 â€” Decompositions & Geometry
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: SVD and its applications; orthogonal projections; analytic geometry (distances, angles, hyperplanes).
- âœ… Pass: Compute SVD; reconstruct matrix from top-k components and plot reconstruction error vs k; project points onto a subspace; compute distances to hyperplanes.
- ğŸ› ï¸ How: `np.linalg.svd`; projection formula; `np.linalg.lstsq` for least squares via normal equations and QR.

Week 12 â€” Vector Calculus I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Partial derivatives; gradients of scalar functions; Jacobians of vector functions.
- âœ… Pass: Compute gradients analytically for multivariate functions; verify with numerical finite differences; visualize gradient field on a contour plot.
- ğŸ› ï¸ How: Derive gradient by hand; implement central differences `(f(x+h)-f(x-h))/(2h)`; `plt.contour` with `plt.quiver`.

Week 13 â€” Vector Calculus II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Chain rule for composed functions; backpropagation intuition; Hessians and second-order derivatives.
- âœ… Pass: Derive gradients of composed functions using chain rule; compute Hessian matrix; verify gradient computation with central-difference check (max abs diff < 1e-4).
- ğŸ› ï¸ How: Symbolic differentiation by hand; numerical Hessian via finite differences; check gradient correctness.

Week 14 â€” Probability I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Random variables; probability distributions (discrete and continuous); expectation and variance; common distributions (Bernoulli, Binomial, Gaussian).
- âœ… Pass: Simulate samples from common distributions; compute empirical vs theoretical mean/variance; verify Law of Large Numbers by plotting sample mean convergence.
- ğŸ› ï¸ How: `np.random`, `scipy.stats`; compare empirical moments to closed-form expressions.

Week 15 â€” Probability II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Joint and marginal distributions; covariance and correlation; multivariate Gaussian; Gaussian conditioning and marginalization.
- âœ… Pass: Generate correlated Normals via Cholesky decomposition; recover empirical covariance matrix; visualize 2D Gaussian contours; demonstrate conditioning a multivariate Gaussian.
- ğŸ› ï¸ How: `L = np.linalg.cholesky(Sigma)`; `X = Z @ L.T`; `np.cov`; contour plots for bivariate Gaussian.

Week 16 â€” Optimization I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Convex functions and sets; gradient descent algorithm; learning rate selection; convergence analysis.
- âœ… Pass: Implement gradient descent for a convex quadratic f(x)=Â½x^TQx+c^Tx; show monotone loss decrease; experiment with different step sizes and plot convergence curves.
- ğŸ› ï¸ How: Analytic gradient Qx+c; fixed and adaptive step sizes; plot loss vs iterations.

Week 17 â€” Optimization II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Newton's method; constrained optimization concepts; regularization and its geometric interpretation.
- âœ… Pass: Implement Newton's method using Hessian; compare convergence (iterations to tolerance) with gradient descent; solve ridge regression and visualize how Î» affects the solution.
- ğŸ› ï¸ How: Newton step: x_new = x - Hâ»Â¹âˆ‡f; `scipy.optimize.minimize`; compare first-order vs second-order methods.

Week 18 â€” Review
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Create concept map linking all MML topics; write summary notes connecting math foundations to ML applications.
- âœ… Pass: A one-page concept map with â‰¥10 explicit connections between math concepts and ML techniques (e.g., eigenvalues â†” PCA, gradient descent â†” neural network training, condition number â†” numerical stability).
- ğŸ› ï¸ How: Use mind-mapping tool or hand-drawn diagram; include concrete examples for each link.
</details>

ğŸ” Flex â€” Retrieval practice and summaries

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 11 Â· Convex Optimization â€” Weeks 122â€“131 (Complete Boyd & Vandenberghe)</b></summary>

Week 122 â€” Mathematical Foundations & Convex Sets
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Understand vector spaces, norms, and topology; master convex sets (definition, operations, separating hyperplanes); work with cones (proper, dual); understand convex hulls and CarathÃ©odory's theorem.
- âœ… Pass: Prove convexity of specific sets analytically; verify convexity numerically for given sets; implement separating hyperplane algorithm; compute convex hull of finite point set; visualize 2D/3D convex sets and their intersections; verify that intersection of convex sets is convex through examples.
- ğŸ› ï¸ How: Check convexity: for x, y in set and Î¸ âˆˆ [0,1], verify Î¸x + (1-Î¸)y in set; `scipy.spatial.ConvexHull`; plot with `plt.fill` for 2D, `mpl_toolkits.mplot3d` for 3D; separating hyperplane via linear program or support vector methods.

Week 123 â€” Convex Functions & Operations
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Define and verify convex functions; understand epigraphs and sublevel sets; master operations preserving convexity (nonnegative weighted sum, composition, pointwise maximum, perspective); work with conjugate functions and Fenchel duality.
- âœ… Pass: Verify convexity via first-order condition (gradient) and second-order condition (Hessian PSD); compute epigraphs and sublevel sets; prove convexity of composed functions using composition rules; compute conjugate functions for common functions (norms, indicators, quadratics); visualize convex functions and their conjugates; implement perspective operation.
- ğŸ› ï¸ How: First-order: `f(y) â‰¥ f(x) + âˆ‡f(x)áµ€(y-x)` for all x,y; second-order: `âˆ‡Â²f(x) âª° 0`; check eigenvalues `np.linalg.eigvals(H) â‰¥ 0`; conjugate: `f*(y) = sup_x(yáµ€x - f(x))`; 3D surface plots for visualization.

Week 124 â€” Convex Optimization Problems
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Formulate optimization problems in standard form; understand linear programming (LP), quadratic programming (QP), and second-order cone programming (SOCP); work with geometric programming; understand quasiconvex optimization.
- âœ… Pass: Formulate â‰¥5 real-world problems as convex programs (portfolio optimization, LP relaxation, robust optimization, etc.); solve using CVX/CVXPY; verify optimality conditions; convert non-convex problems to convex via transformation (log transform for GP); demonstrate equivalence of problem formulations.
- ğŸ› ï¸ How: `cvxpy` for modeling: `cp.Variable`, `cp.Minimize/Maximize`, `cp.Problem(objective, constraints).solve()`; verify KKT conditions at solution; transformations: log-transform for geometric programs; compare solution time across formulations.

Week 125 â€” Duality Theory
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Derive Lagrange dual function and dual problem; understand weak and strong duality; apply Slater's condition for strong duality; work with KKT conditions for optimality; interpret dual variables as sensitivity (shadow prices).
- âœ… Pass: Formulate Lagrangian for â‰¥3 optimization problems; derive dual problem; verify weak duality (dual objective â‰¤ primal objective); check Slater's condition and confirm strong duality; solve primal and dual numerically and verify zero duality gap; interpret dual variables and verify sensitivity interpretation via perturbation analysis; verify KKT conditions at optimum.
- ğŸ› ï¸ How: Lagrangian: `L(x,Î»,Î½) = f(x) + Î£Î»áµ¢gáµ¢(x) + Î£Î½â±¼hâ±¼(x)`; dual function: `g(Î»,Î½) = inf_x L(x,Î»,Î½)`; solve primal/dual with `cvxpy`; access dual variables: `constraint.dual_value`; perturbation: resolve with modified constraint bounds, compare optimal values to dual variables.

Week 126 â€” Unconstrained Optimization Algorithms
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Implement gradient descent with exact and backtracking line search; implement Newton's method and analyze convergence rates; understand quasi-Newton methods (BFGS); compare first-order vs second-order methods.
- âœ… Pass: Implement gradient descent with backtracking line search from scratch; implement Newton's method with Hessian modification for non-convexity; compare convergence rates empirically (linear for GD, quadratic for Newton); implement BFGS and compare to exact Newton; plot objective value, gradient norm, and step size vs iterations; verify theoretical convergence rates on quadratic problems.
- ğŸ› ï¸ How: GD with backtracking: start with step size t, while `f(x - tâˆ‡f) > f(x) - Î±t||âˆ‡f||Â²` do `t = Î²t` (Î±=0.3, Î²=0.8); Newton: `x := x - [âˆ‡Â²f(x)]â»Â¹âˆ‡f(x)`; Hessian modification: add Î»I if not PD; BFGS: update inverse Hessian approximation; `scipy.optimize.minimize(method='BFGS')` for comparison.

Week 127 â€” Equality Constrained Optimization
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Solve equality-constrained problems via elimination and KKT system; implement Newton's method for equality-constrained problems; understand feasible and infeasible start methods.
- âœ… Pass: Solve equality-constrained QP by forming and solving KKT system directly; implement Newton step for equality constraints (compute search direction solving KKT system); compare elimination method (reduce dimensions) vs Lagrange multiplier method; implement feasible start Newton (project onto feasible set) and infeasible start Newton (minimize feasibility and optimality); verify that solution satisfies primal and dual feasibility.
- ğŸ› ï¸ How: KKT system: `[H Aáµ€; A 0][Î”x; Î”Î½] = [-âˆ‡f; -h]` where Ax=b are equality constraints; solve with `np.linalg.solve`; elimination: express x = Fz + xâ‚€ where Fxâ‚€=b, AF=0, then minimize in z; feasibility measure: `||Ax-b||Â²`; verify solution: check `Ax=b` and `âˆ‡f + Aáµ€Î½ = 0`.

Week 128 â€” Interior-Point Methods
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Understand barrier methods and central path; implement log-barrier method for inequality constraints; understand primal-dual interior-point methods; analyze complexity and convergence.
- âœ… Pass: Implement log-barrier method for LP or QP with inequality constraints; track central path by solving sequence of problems for decreasing t; implement primal-dual interior-point method computing Newton steps in primal-dual space; compare to barrier method; plot duality gap vs iterations; verify polynomial-time complexity empirically; compare to simplex method for LP.
- ğŸ› ï¸ How: Barrier function: `Ï†(x) = -Î£ log(-fáµ¢(x))`; minimize `tÂ·fâ‚€(x) + Ï†(x)` for increasing t; primal-dual: solve KKT system with perturbed complementarity `Î»áµ¢fáµ¢(x) = -1/t`; Newton step: `[H+âˆ‡Â²Ï† Aáµ€; A 0][Î”x;Î”Î½] = [-tâˆ‡f-âˆ‡Ï†; -Ax+b]`; track `Î· = m/t` (duality gap upper bound).

Week 129 â€” Applications to Machine Learning
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Formulate ML problems as convex optimization: SVM (hinge loss, soft margin), logistic regression, Lasso and elastic net, matrix completion, robust PCA; understand regularization from optimization perspective.
- âœ… Pass: Formulate and solve SVM dual problem; implement coordinate descent for Lasso; formulate logistic regression as convex problem and solve with Newton's method; implement matrix completion via nuclear norm minimization; solve robust PCA (low-rank + sparse decomposition); compare custom implementations to sklearn baselines; visualize regularization paths and decision boundaries.
- ğŸ› ï¸ How: SVM dual: `max Î£Î±áµ¢ - Â½Î£Î£Î±áµ¢Î±â±¼yáµ¢yâ±¼K(xáµ¢,xâ±¼)` subject to `0 â‰¤ Î± â‰¤ C`, `Î£Î±áµ¢yáµ¢=0`; Lasso coordinate descent: update one coefficient at a time with soft thresholding; nuclear norm: `||X||* = Î£Ïƒáµ¢`; robust PCA: `min ||L||* + Î»||S||â‚` subject to `L+S=M`; use `cvxpy` for verification.

Week 130 â€” Advanced Topics: Distributed & Stochastic Methods
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Understand decomposition methods (dual decomposition, ADMM); implement stochastic gradient methods; work with proximal operators and proximal gradient method; understand operator splitting methods.
- âœ… Pass: Implement ADMM for a separable problem (e.g., Lasso, consensus optimization); implement stochastic gradient descent with diminishing and constant step sizes; compare convergence to batch GD; implement proximal gradient method for composite objectives (smooth + nonsmooth); derive and implement proximal operators for common functions (L1 norm, indicator functions); demonstrate ADMM convergence to consensus.
- ğŸ› ï¸ How: ADMM: iterate `x := argmin L_Ï(x,z,u)`, `z := argmin L_Ï(x,z,u)`, `u := u + Ï(Ax+Bz-c)` where `L_Ï = f(x)+g(z)+uáµ€(Ax+Bz-c)+Ï/2||Ax+Bz-c||Â²`; SGD: sample minibatch, update with gradient estimate; proximal operator: `prox_f(x) = argmin_u (f(u) + Â½||u-x||Â²)`; proximal gradient: `x := prox_{tg}(x - tâˆ‡f(x))`.

Week 131 â€” Integration & Review
- ğŸ“– [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- ğŸ§ª Practice: Integrate all topics; formulate and solve complex real-world problems; understand when to use each algorithm; compare optimization formulations.
- âœ… Pass: Complete a comprehensive project applying convex optimization to a real problem; formulate in â‰¥2 different ways (primal/dual, different variables); solve with â‰¥3 algorithms comparing convergence and computation time; verify optimality via KKT conditions and duality gap; produce detailed report documenting problem formulation, algorithm selection rationale, convergence analysis, and sensitivity analysis; include visualizations of feasible set, level sets, and optimization trajectory.
- ğŸ› ï¸ How: Select problem from application domain (portfolio, control, signal processing, ML); compare custom implementations to industrial solvers (CVXPY, Gurobi, MOSEK); profiling with `cProfile` or `line_profiler`; convergence plots (objective, constraint violation, KKT residual); sensitivity: perturb problem data and track optimal value.
</details>

ğŸ” Flex â€” Convex optimization consolidation

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 3 Â· Statistics Fundamentals â€” Weeks 19â€“24 (Complete Think Stats)</b></summary>

Week 19 â€” Think Stats Ch. 1
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Explore a dataset; compute summary statistics; build histograms and PMFs; construct ECDFs.
- âœ… Pass: Implement ECDF from scratch on real data; verify it is non-decreasing and ends at 1.0; overlay histogram and ECDF to compare distributional insights; interpret outliers.
- ğŸ› ï¸ How: `np.sort`; `np.arange(1,n+1)/n`; `plt.step` for ECDF; `plt.hist` for histogram.

Week 20 â€” Think Stats Ch. 2
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Compute central tendency (mean, median, mode) and spread (variance, std, range, IQR); explore effect of outliers on these measures.
- âœ… Pass: Compare mean/SD vs median/MAD/IQR on 2 datasets (one symmetric, one skewed); explain when each measure is appropriate; show outlier impact graphically.
- ğŸ› ï¸ How: `np.mean`, `np.median`, `np.std`; `scipy.stats.median_abs_deviation`; `np.percentile` for IQR.

Week 21 â€” Think Stats Ch. 3â€“4
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Work with CDFs and PMFs; model data with probability distributions; compare empirical vs theoretical distributions.
- âœ… Pass: Fit data to common distributions (Normal, Exponential); use CDF plots to assess fit; compute percentiles and quantiles; explain when to use PMF vs CDF.
- ğŸ› ï¸ How: `scipy.stats.norm.fit`, `scipy.stats.expon`; `probplot` for QQ plots; CDF comparison plots.

Week 22 â€” Think Stats Ch. 5â€“6
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Model data with analytical distributions; explore relationships between variables; compute conditional probabilities.
- âœ… Pass: Fit a parametric model to real data; compute and interpret correlation and covariance; demonstrate conditional probability with a contingency table.
- ğŸ› ï¸ How: `scipy.stats` distribution fitting; `np.corrcoef`; `pd.crosstab` for contingency tables.

Week 23 â€” Think Stats Ch. 7â€“8
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Estimate parameters with confidence intervals; perform hypothesis tests; understand p-values and statistical significance.
- âœ… Pass: Compute confidence intervals via bootstrap and analytical methods; run a hypothesis test; simulate to show Type I error â‰ˆ Î±; produce a power curve for detecting effect sizes.
- ğŸ› ï¸ How: Bootstrap resampling; `scipy.stats.ttest_ind`; simulation to count rejections under Hâ‚€ and Hâ‚.

Week 24 â€” Think Stats Ch. 9â€“10 (+wrap)
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Explore linear relationships; fit simple and multiple regression; interpret coefficients; check regression assumptions.
- âœ… Pass: Fit OLS regression; interpret RÂ², coefficients, and p-values; produce diagnostic plots (residuals vs fitted, QQ plot); compute VIFs and flag multicollinearity.
- ğŸ› ï¸ How: `statsmodels.api.OLS`; `statsmodels.stats.outliers_influence.variance_inflation_factor`; diagnostic plots.
</details>

ğŸ” Flex â€” Stats recap

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 4 Â· Applied Multivariate Statistics â€” Weeks 25â€“39 (Complete PSU STAT 505)</b></summary>

Week 25 â€” Measures of Central Tendency, Dispersion and Association (Lesson 1)
- ğŸ“– Activities: [PSU STAT 505 Lesson 1](https://online.stat.psu.edu/stat505/lesson/1)
- ğŸ§ª Practice: Compute multivariate measures of central tendency (mean vectors); understand dispersion (covariance matrices, generalized variance); compute association measures (correlation matrices); interpret relationships between variables.
- âœ… Pass: Calculate mean vectors and covariance matrices for multivariate data; compute correlation matrices and interpret relationships; calculate generalized variance and total variation; compare variance-covariance structures across groups. Complete PSU STAT 505 Lesson 1.
- ğŸ› ï¸ How: `np.mean(axis=0)` for mean vectors; `np.cov` for covariance matrices; `np.corrcoef` for correlation; `np.linalg.det` for generalized variance; visualize with heatmaps.

Week 26 â€” Linear Combinations of Random Variables (Lesson 2)
- ğŸ“– Activities: [PSU STAT 505 Lesson 2](https://online.stat.psu.edu/stat505/lesson/2)
- ğŸ§ª Practice: Understand properties of linear combinations of random vectors; compute means and covariances of linear combinations; work with linear transformations; understand independence and correlation.
- âœ… Pass: Compute mean and covariance of linear combinations; verify properties of linear transformations; demonstrate how linear combinations preserve or change correlation structure; apply to dimensionality reduction scenarios. Complete PSU STAT 505 Lesson 2.
- ğŸ› ï¸ How: Matrix operations for linear combinations `Y = AX + b`; compute `E[Y] = AE[X] + b` and `Cov(Y) = A Cov(X) A^T`; verify independence conditions; visualize transformations.

Week 27 â€” Graphical Display of Multivariate Data (Lesson 3)
- ğŸ“– Activities: [PSU STAT 505 Lesson 3](https://online.stat.psu.edu/stat505/lesson/3)
- ğŸ§ª Practice: Create various multivariate visualizations; produce scatter plot matrices, star plots, profile plots; understand 3D plots and contour plots; interpret patterns and relationships visually.
- âœ… Pass: Create comprehensive visualization suite: scatter plot matrix with correlations, 3D scatter plots, profile plots for repeated measures, star plots for multivariate observations, contour plots for bivariate distributions; identify patterns, outliers, and relationships. Complete PSU STAT 505 Lesson 3.
- ğŸ› ï¸ How: `pd.plotting.scatter_matrix`; `mpl_toolkits.mplot3d` for 3D plots; `plt.plot` for profile plots; `seaborn.pairplot`; custom star/radar plots with `plt.subplot(projection='polar')`.

Week 28 â€” Multivariate Normal Distribution (Lesson 4)
- ğŸ“– Activities: [PSU STAT 505 Lesson 4](https://online.stat.psu.edu/stat505/lesson/4)
- ğŸ§ª Practice: Understand multivariate normal distribution properties; compute Mahalanobis distance; generate samples from MVN; test for multivariate normality; understand conditional and marginal distributions.
- âœ… Pass: Generate samples from multivariate normal; compute and interpret Mahalanobis distance vs Euclidean distance; perform Mardia's test for multivariate normality; compute marginal and conditional distributions; visualize MVN with contour plots. Complete PSU STAT 505 Lesson 4.
- ğŸ› ï¸ How: `scipy.stats.multivariate_normal`; `scipy.spatial.distance.mahalanobis`; Mardia's skewness and kurtosis tests; compute conditionals using partitioned covariance matrices.

Week 29 â€” Sample Mean Vector and Sample Correlation (Lesson 5)
- ğŸ“– Activities: [PSU STAT 505 Lesson 5](https://online.stat.psu.edu/stat505/lesson/5)
- ğŸ§ª Practice: Estimate mean vectors and covariance matrices from samples; understand sampling distributions; perform inference on mean vectors; test hypotheses about correlations; construct confidence regions.
- âœ… Pass: Estimate mean vectors and covariance matrices; derive sampling distributions; construct confidence ellipses for mean vectors; test hypotheses about population means; compute standard errors for correlations. Complete PSU STAT 505 Lesson 5.
- ğŸ› ï¸ How: Sample statistics with `np.mean`, `np.cov`; Wishart distribution for covariance; confidence ellipses using eigenvalues/eigenvectors; bootstrap for inference.

Week 30 â€” Multivariate Conditional Distribution and Partial Correlation (Lesson 6)
- ğŸ“– Activities: [PSU STAT 505 Lesson 6](https://online.stat.psu.edu/stat505/lesson/6)
- ğŸ§ª Practice: Compute conditional distributions from joint multivariate normal; calculate partial correlations; understand the difference between marginal and partial correlation; interpret conditional independence.
- âœ… Pass: Partition covariance matrices to compute conditional distributions; calculate and interpret partial correlations; compare partial vs marginal correlations; test for conditional independence; visualize relationships controlling for other variables. Complete PSU STAT 505 Lesson 6.
- ğŸ› ï¸ How: Use partitioned covariance matrices `Î£ = [[Î£11, Î£12], [Î£21, Î£22]]`; conditional mean `Î¼1 + Î£12 Î£22^-1 (x2 - Î¼2)`; conditional covariance `Î£11 - Î£12 Î£22^-1 Î£21`; `pingouin.partial_corr` for partial correlations.

Week 31 â€” Inferences Regarding Multivariate Population Mean (Lesson 7)
- ğŸ“– Activities: [PSU STAT 505 Lesson 7](https://online.stat.psu.edu/stat505/lesson/7)
- ğŸ§ª Practice: Perform Hotelling's TÂ² tests for one-sample and two-sample problems; construct simultaneous confidence intervals; understand multivariate hypothesis testing; compare with univariate t-tests.
- âœ… Pass: Conduct one-sample Hotelling's TÂ² test; perform two-sample Hotelling's TÂ² test; construct simultaneous confidence intervals using Bonferroni correction; compare multivariate vs univariate approaches; interpret test statistics, p-values, and effect sizes. Complete PSU STAT 505 Lesson 7.
- ğŸ› ï¸ How: Implement `TÂ² = n(xÌ„ - Î¼0)^T S^-1 (xÌ„ - Î¼0)`; convert to F-statistic: `F = (n-p)TÂ²/((n-1)p)`; `scipy.stats.f` for p-values; Bonferroni intervals: `t_(Î±/2p, n-1)`.

Week 32 â€” Multivariate Analysis of Variance (MANOVA) (Lesson 8)
- ğŸ“– Activities: [PSU STAT 505 Lesson 8](https://online.stat.psu.edu/stat505/lesson/8)
- ğŸ§ª Practice: Perform one-way and two-way MANOVA; understand Wilks' Lambda, Pillai's trace, and other test statistics; conduct post-hoc tests; check MANOVA assumptions; compare to univariate ANOVA.
- âœ… Pass: Run MANOVA with â‰¥2 dependent variables and â‰¥3 groups; report test statistics (Wilks' Lambda, Pillai's trace, Hotelling-Lawley trace, Roy's largest root); perform follow-up univariate ANOVAs and discriminant analysis; check assumptions (Box's M test, multivariate normality). Complete PSU STAT 505 Lesson 8.
- ğŸ› ï¸ How: `statsmodels.multivariate.manova.MANOVA`; interpret output; visualize group centroids; check assumptions before interpretation; compare effect sizes across responses.

Week 33 â€” Repeated Measures Analysis (Lesson 9)
- ğŸ“– Activities: [PSU STAT 505 Lesson 9](https://online.stat.psu.edu/stat505/lesson/9)
- ğŸ§ª Practice: Analyze repeated measures data using multivariate approach; understand sphericity and compound symmetry; perform profile analysis; test for parallelism, coincidence, and flatness; handle within-subject factors.
- âœ… Pass: Analyze repeated measures design with multivariate approach; test sphericity assumption (Mauchly's test); perform profile analysis testing parallelism, levels, and flatness hypotheses; compare multivariate vs univariate repeated measures ANOVA; interpret within-subject and between-subject effects. Complete PSU STAT 505 Lesson 9.
- ğŸ› ï¸ How: `statsmodels` for repeated measures; test sphericity; profile plots with error bars; Greenhouse-Geisser correction when sphericity violated; contrast matrices for specific comparisons.

Week 34 â€” Discriminant Analysis (Lesson 10)
- ğŸ“– Activities: [PSU STAT 505 Lesson 10](https://online.stat.psu.edu/stat505/lesson/10)
- ğŸ§ª Practice: Perform linear and quadratic discriminant analysis; understand Fisher's linear discriminant; classify observations; evaluate classification performance; understand relationship to MANOVA; compare LDA/QDA assumptions.
- âœ… Pass: Apply LDA and QDA to classification problem; compute discriminant functions and classify held-out observations; report confusion matrix and misclassification rates; visualize decision boundaries; compare LDA/QDA to logistic regression; verify equal covariance assumption. Complete PSU STAT 505 Lesson 10.
- ğŸ› ï¸ How: `sklearn.discriminant_analysis.LinearDiscriminantAnalysis/QuadraticDiscriminantAnalysis`; `classification_report`; ROC curves; cross-validation for error estimation; Box's M test for covariance equality.

Week 35 â€” Principal Components Analysis (Lesson 11)
- ğŸ“– Activities: [PSU STAT 505 Lesson 11](https://online.stat.psu.edu/stat505/lesson/11)
- ğŸ§ª Practice: Perform PCA on correlation and covariance matrices; understand eigenvalues/eigenvectors interpretation; determine number of components; compute component scores; interpret loadings; create biplots; understand variance explained.
- âœ… Pass: Apply PCA to dataset with â‰¥6 variables; create scree plot; select components using Kaiser criterion (eigenvalue > 1) and cumulative variance (80%); interpret loadings for first 2-3 PCs; create biplot; reconstruct data; compare PCA on correlation vs covariance. Complete PSU STAT 505 Lesson 11.
- ğŸ› ï¸ How: `sklearn.decomposition.PCA`; standardize with `StandardScaler`; `explained_variance_ratio_`; scree plot; biplot with `plt.arrow`; verify reconstruction error.

Week 36 â€” Factor Analysis (Lesson 12)
- ğŸ“– Activities: [PSU STAT 505 Lesson 12](https://online.stat.psu.edu/stat505/lesson/12)
- ğŸ§ª Practice: Perform exploratory factor analysis; understand factor model and common vs specific variance; estimate communalities and uniqueness; perform factor rotations (varimax, promax); determine number of factors; interpret factor loadings.
- âœ… Pass: Conduct factor analysis; determine number of factors using parallel analysis and scree plot; extract factors using maximum likelihood or principal axis factoring; perform varimax and promax rotations; interpret and name factors; report communalities and variance explained; compare to PCA. Complete PSU STAT 505 Lesson 12.
- ğŸ› ï¸ How: `sklearn.decomposition.FactorAnalysis`; `factor_analyzer` package for rotations; parallel analysis comparing eigenvalues to random data; factor loading interpretation with cutoff |loading| > 0.3.

Week 37 â€” Canonical Correlation Analysis (Lesson 13)
- ğŸ“– Activities: [PSU STAT 505 Lesson 13](https://online.stat.psu.edu/stat505/lesson/13)
- ğŸ§ª Practice: Perform canonical correlation analysis between two sets of variables; compute canonical correlations and canonical variates; test significance; interpret canonical loadings and cross-loadings; assess redundancy.
- âœ… Pass: Apply CCA to dataset with two variable sets (â‰¥3 variables each); compute all canonical correlations and test significance; interpret first 2-3 canonical variate pairs; compute canonical loadings (structure correlations); perform redundancy analysis; visualize canonical variates. Complete PSU STAT 505 Lesson 13.
- ğŸ› ï¸ How: `sklearn.cross_decomposition.CCA`; Wilks' Lambda test: `Î› = âˆ(1 - rÂ²)`; canonical loadings as correlations between original variables and canonical variates; redundancy index.

Week 38 â€” Cluster Analysis (Lesson 14)
- ğŸ“– Activities: [PSU STAT 505 Lesson 14](https://online.stat.psu.edu/stat505/lesson/14)
- ğŸ§ª Practice: Apply hierarchical clustering with different linkage methods; perform k-means clustering; understand distance measures and similarity metrics; determine optimal number of clusters; validate clustering solutions; compare clustering methods.
- âœ… Pass: Perform hierarchical clustering with â‰¥3 linkage methods (single, complete, average, Ward); create dendrograms; apply k-means with multiple k values; determine optimal k using elbow method, silhouette analysis, and gap statistic; validate with silhouette scores; visualize clusters using PCA; compare hierarchical vs partitioning methods. Complete PSU STAT 505 Lesson 14.
- ğŸ› ï¸ How: `scipy.cluster.hierarchy` for hierarchical clustering; `sklearn.cluster.KMeans`; distance metrics: Euclidean, Manhattan, Mahalanobis; silhouette analysis; dendrogram interpretation; standardize data before clustering.

Week 39 â€” Integration and Review
- ğŸ“– Activities: Review all PSU STAT 505 lessons
- ğŸ§ª Practice: Integrate multivariate methods in comprehensive analysis; understand when to use each technique; compare and contrast methods; apply multiple techniques to same dataset.
- âœ… Pass: Complete end-to-end multivariate analysis applying â‰¥5 techniques from course; write comprehensive report connecting methods; explain method selection rationale; interpret results in context; discuss assumptions and limitations; compare insights from different methods.
- ğŸ› ï¸ How: Choose appropriate methods for research question; check assumptions; compare complementary analyses (e.g., PCA then cluster analysis; MANOVA then discriminant analysis); synthesize findings.
</details>

ğŸ” Flex â€” Multivariate stats consolidation

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 5 Â· Bayesian & Missing Data â€” Weeks 40â€“51 (Complete Think Bayes, FIMD)</b></summary>

Weeks 40â€“47 â€” Think Bayes (Ch. 1â€“14, paced)
- ğŸ“– [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- ğŸ§ª Practice: Apply Bayes' theorem to update beliefs; implement conjugate prior models (Beta-Binomial, Gamma-Poisson, Normal-Normal); perform posterior predictive checks; compare models.
- âœ… Pass (weekly): Implement a Bayesian model aligned with the chapter's topic; show prior sensitivity analysis (vary prior parameters and observe posterior changes); generate posterior predictive samples and compare to observed data using a suitable test statistic.
- ğŸ› ï¸ How: Use analytical posteriors when available; for PPC, draw samples from posterior, then from likelihood, and compare summary stats to data.

Weeks 48â€“51 â€” Flexible Imputation of Missing Data (complete)
- ğŸ“– [FIMD](https://stefvanbuuren.name/fimd/)
- ğŸ§ª Practice: Missingness mechanisms; MICE; sensitivity (as in book)
- âœ… Pass (weekly): Run MICE (mâ‰¥5) on a dataset; report pooled estimates per Rubinâ€™s rules; compare to complete-case; perform delta-adjustment sensitivity where relevant.
- ğŸ› ï¸ How: use a MICE implementation (e.g., statsmodels/impyute/sklearn-iterative as proxy) consistent with book procedures.
</details>

ğŸ” Flex â€” Consolidate Bayesian + MI

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 6 Â· Statistical Learning with Python â€” Weeks 52â€“61 (Complete ISLP)</b></summary>

Week 52 â€” ISLP Ch. 1â€“2 (Intro + Statistical Learning)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Understand the statistical learning framework; implement train/test splits; explore the bias-variance trade-off with KNN at different k values.
- âœ… Pass: On a dataset, demonstrate how training error decreases with model complexity while test error shows U-shape; implement 5-fold CV and compare to hold-out estimate; discuss flexibility vs interpretability.
- ğŸ› ï¸ How: `train_test_split`; `KFold`/`cross_val_score`; vary KNN's k parameter; plot training vs test error curves.

Week 53 â€” ISLP Ch. 3 (Linear Regression)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Fit simple and multiple linear regression; interpret coefficients; add interaction and polynomial terms; assess model fit with residual diagnostics.
- âœ… Pass: Fit OLS with and without interaction/polynomial terms; compare RÂ² vs adjusted RÂ²; produce residual plots; select optimal polynomial degree via CV; interpret coefficient confidence intervals.
- ğŸ› ï¸ How: `LinearRegression`; `PolynomialFeatures`; `cross_val_score`; `statsmodels` for CIs; residual diagnostics.

Week 54 â€” ISLP Ch. 4 (Classification)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Implement logistic regression; understand LDA/QDA assumptions; apply KNN for classification; explore classification metrics beyond accuracy.
- âœ… Pass: Compare logistic regression, LDA, QDA, and KNN using stratified 5-fold CV; report confusion matrix, precision, recall, and ROC-AUC; select optimal classification threshold based on problem context.
- ğŸ› ï¸ How: `LogisticRegression`; `LinearDiscriminantAnalysis`; `QuadraticDiscriminantAnalysis`; `KNeighborsClassifier`; `roc_curve` for threshold selection.

Week 55 â€” ISLP Ch. 5 (Resampling Methods)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Compare validation approaches: hold-out, LOOCV, k-fold CV; use bootstrap for uncertainty estimation; understand variance-bias trade-off in resampling.
- âœ… Pass: Compare test error estimates from LOOCV vs 5-fold vs 10-fold CV; implement bootstrap to estimate coefficient standard errors; compare bootstrap SEs to analytic SEs.
- ğŸ› ï¸ How: `LeaveOneOut`; `KFold`; implement bootstrap loop with `np.random.choice`; fix seeds for reproducibility.

Week 56 â€” ISLP Ch. 6 (Model Selection & Regularization)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Understand the motivation for regularization; implement ridge and lasso regression; interpret coefficient shrinkage and sparsity; tune regularization parameter via CV.
- âœ… Pass: Plot ridge and lasso coefficient paths as Î» varies; select optimal Î» via CV; compare test error of OLS vs ridge vs lasso; explain when lasso produces sparse solutions.
- ğŸ› ï¸ How: `Ridge`; `Lasso`; `RidgeCV`; `LassoCV`; `StandardScaler` (scale features first); `lasso_path` for path plots.

Week 57 â€” ISLP Ch. 7 (Beyond Linearity)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Move beyond linearity with polynomial regression, step functions, and splines; understand degrees of freedom; fit GAM-style models.
- âœ… Pass: Fit polynomial, step function, and spline models; compare their flexibility and test errors; produce partial dependence plots; select appropriate number of knots/degrees via CV.
- ğŸ› ï¸ How: `PolynomialFeatures`; `SplineTransformer`; `pd.cut` for step functions; compare MSE on held-out data.

Week 58 â€” ISLP Ch. 8 (Tree-Based Methods)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Fit decision trees; understand bagging and the random forest algorithm; implement gradient boosting; interpret tree-based models.
- âœ… Pass: Fit and prune a decision tree; compare single tree vs random forest vs gradient boosting on test error; show OOB error for RF; plot feature importances and partial dependence plots.
- ğŸ› ï¸ How: `DecisionTreeClassifier/Regressor`; `RandomForestClassifier/Regressor`; `GradientBoostingClassifier/Regressor`; `permutation_importance`; `plot_partial_dependence`.

Week 59 â€” ISLP Ch. 9 (Support Vector Machines)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Understand maximal margin classifiers and support vectors; fit SVMs with linear and non-linear kernels; tune hyperparameters (C, gamma).
- âœ… Pass: Fit SVM with linear and RBF kernels; tune C and gamma via grid search with CV; visualize decision boundaries on 2D data; identify and highlight support vectors; compare to logistic regression.
- ğŸ› ï¸ How: `SVC`; `GridSearchCV`; `plt.contourf` for decision boundaries; access `support_vectors_` attribute.

Week 60 â€” ISLP Ch. 10 (Unsupervised Learning)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Perform dimensionality reduction with PCA; apply k-means and hierarchical clustering; understand the importance of scaling; evaluate clustering quality.
- âœ… Pass: Apply PCA and plot cumulative explained variance; choose number of components; cluster with k-means (elbow method for k) and hierarchical clustering (dendrogram); evaluate with silhouette score and compare cluster stability across random seeds.
- ğŸ› ï¸ How: `StandardScaler` (always scale first); `PCA`; `KMeans` with inertia plots; `AgglomerativeClustering`; `dendrogram`; `silhouette_score`.

Week 61 â€” ISLP Labs/Wrap-up
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Complete an end-to-end ML project using techniques from all ISLP chapters: EDA, preprocessing, model selection, hyperparameter tuning, evaluation, and interpretation.
- âœ… Pass: Deliver a reproducible notebook with proper train/test split, cross-validation, model comparison, hyperparameter tuning, error analysis, and a 1-page summary documenting decisions, limitations, and risks.
- ğŸ› ï¸ How: `Pipeline`; `ColumnTransformer` for mixed feature types; `GridSearchCV`/`RandomizedSearchCV`; fixed `random_state` throughout; clean documentation.
</details>

ğŸ” Flex â€” Validation basics consolidation

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 8 Â· Data Mining â€” Weeks 81â€“89 (Complete DM 3e)</b></summary>

Weeks 81â€“89 â€” Data Mining 3e (Ch. 1â€“12)
- ğŸ“– [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- ğŸ§ª Practice: Per-chapter algorithmic work strictly matching the chapter (e.g., data preprocessing tasks; Apriori/FP-Growth; decision trees; k-means/DBSCAN; outlier detection)
- âœ… Pass (weekly): Implement a minimal working version for the chapterâ€™s focal algorithm OR replicate results using a library; verify correctness on a deterministic toy and compare performance on a small real dataset.
- ğŸ› ï¸ How: construct small synthetic datasets with known ground truth (fixed seeds); assert counts/clusters/rules match expectation.
</details>

ğŸ” Flex â€” Mining recap

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 7 Â· Classical ML â€” Weeks 62â€“80 (Complete PRML, Interpretable ML)</b></summary>

Weeks 62â€“75 â€” PRML (Ch. 1â€“13 + review)
- ğŸ“– [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- ğŸ§ª Practice: Implement core algorithms from each chapter from scratch: probability distributions, linear models, neural networks, kernel methods, graphical models, mixture models, EM algorithm, approximate inference, and sampling methods.
- âœ… Pass (weekly): Implement the chapter's focal algorithm from scratch; verify correctness by comparing to sklearn/scipy baseline (within 2-5% accuracy); document mathematical derivations; use fixed seeds for reproducibility.
- ğŸ› ï¸ How: Use NumPy for implementations; sklearn only as verification oracle; work on toy datasets; keep detailed notes linking code to book equations.

Weeks 76â€“80 â€” Interpretable ML (complete)
- ğŸ“– [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- ğŸ§ª Practice: Apply model-agnostic interpretation methods: PDP, ICE, permutation importance, LIME, SHAP; understand intrinsically interpretable models; explore feature interaction methods.
- âœ… Pass (weekly): For a trained model, produce PDP/ICE plots for top features; compute permutation importance; generate SHAP values for individual predictions; write a 1-page analysis comparing methods' stability across 3 bootstrap resamples.
- ğŸ› ï¸ How: `sklearn.inspection.PartialDependenceDisplay`; `permutation_importance`; `shap.Explainer`; compare explanations across train/test sets.
</details>

ğŸ” Flex â€” Validation & interpretation synthesis

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 12 Â· Elements of Statistical Learning â€” Weeks 132â€“141 (Complete ESL)</b></summary>

Week 132 â€” ESL Ch. 1â€“3: Introduction & Linear Methods
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Review statistical learning framework; master linear regression theory (bias-variance decomposition, Gauss-Markov theorem); implement subset selection, ridge, and lasso from scratch; understand effective degrees of freedom.
- âœ… Pass: Derive bias-variance decomposition analytically; prove Gauss-Markov theorem; implement best subset selection via exhaustive search for p â‰¤ 10; implement ridge and lasso with coordinate descent; compute effective degrees of freedom `df(Î») = tr[X(Xáµ€X + Î»I)â»Â¹Xáµ€]` and verify empirically; compare subset selection, ridge, and lasso on test error and coefficient paths.
- ğŸ› ï¸ How: Bias-variance: `E[(Y-fÌ‚)Â²] = BiasÂ²(fÌ‚) + Var(fÌ‚) + ÏƒÂ²`; Gauss-Markov: show OLS has minimum variance among linear unbiased estimators; subset selection: iterate over all 2^p subsets; ridge: `Î²Ì‚ = (Xáµ€X + Î»I)â»Â¹Xáµ€y`; lasso coordinate descent: soft thresholding `S(z,Î³) = sign(z)(|z|-Î³)â‚Š`; effective df from hat matrix trace.

Week 133 â€” ESL Ch. 4â€“5: Linear Classification & Basis Expansions
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Master linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), logistic regression, and separating hyperplanes; understand basis expansions (polynomial, splines, wavelets); implement natural cubic splines.
- âœ… Pass: Derive LDA decision boundary assuming equal covariance; implement QDA allowing separate covariances; fit logistic regression via Newton-Raphson (IRLS); implement linear separating hyperplane via perceptron algorithm; construct natural cubic spline basis manually and fit regression; compare polynomial vs spline fits showing boundary bias and variance; derive and verify degrees of freedom for smoothing splines.
- ğŸ› ï¸ How: LDA: estimate class means Î¼â‚– and pooled covariance Î£, classify via `argmax_k log P(G=k) - Â½(x-Î¼â‚–)áµ€Î£â»Â¹(x-Î¼â‚–)`; QDA: separate Î£â‚– for each class; logistic IRLS: iterate `Î² := Î² + (Xáµ€WX)â»Â¹Xáµ€(y-p)` where W=diag(p(1-p)); perceptron: `Î² := Î² + Î·yáµ¢xáµ¢` for misclassified points; natural spline: impose constraints for linearity beyond boundary knots.

Week 134 â€” ESL Ch. 6â€“7: Kernel Methods & Model Assessment
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Understand kernel smoothing and local regression; implement k-nearest neighbors, Nadaraya-Watson estimator, local polynomial regression; master cross-validation theory (GCV, leave-one-out shortcuts); understand bootstrap for model selection; derive and implement optimism estimators (Cp, AIC, BIC).
- âœ… Pass: Implement Nadaraya-Watson kernel regression with Gaussian kernel and bandwidth selection via CV; implement local linear regression (LOESS) and show boundary bias correction compared to Nadaraya-Watson; derive and implement leave-one-out CV shortcut for linear smoothers via hat matrix; implement 0.632 bootstrap estimator; compute Cp, AIC, BIC for nested models and verify consistency of BIC; compare all model selection criteria on a common dataset.
- ğŸ› ï¸ How: Nadaraya-Watson: `fÌ‚(xâ‚€) = Î£ K((xáµ¢-xâ‚€)/h)yáµ¢ / Î£ K((xáµ¢-xâ‚€)/h)`; LOESS: weighted least squares in local neighborhood; LOO shortcut: `CV = (1/n)Î£(yáµ¢-fÌ‚(xáµ¢))Â²/(1-háµ¢áµ¢)Â²` where háµ¢áµ¢ is diagonal of hat matrix; Cp: `RSS/ÏƒÂ² + 2d`; AIC: `-2log-likelihood + 2d`; BIC: `-2log-likelihood + log(n)d`.

Week 135 â€” ESL Ch. 8â€“9: Model Inference & Additive Models
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Understand bootstrap for inference (standard errors, confidence intervals, percentile and BCa methods); implement permutation tests; master generalized additive models (GAMs) with backfitting algorithm; understand tree-based models and CART algorithm.
- âœ… Pass: Implement bootstrap confidence intervals (normal, percentile, BCa) and compare coverage on simulations; implement permutation test for independence and verify Type I error rate; implement GAM backfitting algorithm from scratch for additive model with spline components; fit and prune CART tree using cost-complexity pruning; compare tree to GAM on same dataset; prove backfitting convergence for additive models.
- ğŸ› ï¸ How: BCa: bias-correction `zâ‚€` and acceleration `a` from jackknife; percentile: 2.5% and 97.5% quantiles of bootstrap distribution; permutation: shuffle one variable, recompute test statistic; backfitting: iterate `fÌ‚â±¼ := S_j[Y - Î£â‚–â‰ â±¼fÌ‚â‚–]` where Sâ±¼ is smoother; CART: recursive binary splits minimizing RSS or Gini; cost-complexity: `min_T Î£(yáµ¢-Å·â‚œ)Â² + Î±|T|`.

Week 136 â€” ESL Ch. 10: Boosting & Additive Models
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Understand AdaBoost algorithm and its connection to exponential loss; implement gradient boosting from scratch; understand forward stagewise additive modeling; derive L2Boosting and show connection to gradient descent in function space; implement shrinkage and early stopping for regularization.
- âœ… Pass: Implement AdaBoost with decision stumps from scratch; show connection to exponential loss by deriving weight updates; implement gradient boosting with squared loss and deviance loss; demonstrate that gradient boosting is steepest descent in function space; compare learning rates and early stopping for regularization; implement stochastic gradient boosting (subsampling); produce learning curves showing train/validation error vs boosting iterations.
- ğŸ› ï¸ How: AdaBoost: iterate `err_m = Î£w_i I(y_iâ‰ G_m(x_i))/Î£w_i`, `Î±_m = log((1-err_m)/err_m)`, `w_i := w_i exp(Î±_m I(y_iâ‰ G_m))`, final: `G = sign(Î£Î±_m G_m)`; gradient boosting: `f_m = f_{m-1} + Î½Â·h_m` where `h_m` fits residuals `-âˆ‚L/âˆ‚f`; derive for squared loss: residuals are `y-f`; for deviance: residuals are gradients of log-likelihood.

Week 137 â€” ESL Ch. 11â€“12: Neural Networks & Support Vector Machines
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Implement feedforward neural network with backpropagation from scratch; understand universal approximation; derive and implement weight decay and early stopping; implement SVM via quadratic programming; understand kernel trick and mercer kernels; compare SVM to logistic regression and neural networks.
- âœ… Pass: Implement multi-layer perceptron with one hidden layer from scratch including backpropagation; verify gradient computation with finite differences; train on classification and regression tasks with weight decay; implement SVM dual problem and solve with quadratic programming; implement kernel SVM with RBF kernel; visualize decision boundaries; compare SVM, logistic regression, and neural network on nonlinearly separable data; demonstrate kernel trick equivalence.
- ğŸ› ï¸ How: Backprop: forward pass compute activations, backward pass compute gradients via chain rule; weight update: `w := w - Î·âˆ‚L/âˆ‚w`; SVM dual: `max Î£Î±áµ¢ - Â½Î£Î£Î±áµ¢Î±â±¼yáµ¢yâ±¼xáµ¢áµ€xâ±¼` subject to `0â‰¤Î±â‰¤C`, `Î£Î±áµ¢yáµ¢=0`; kernel trick: replace `xáµ¢áµ€xâ±¼` with `K(xáµ¢,xâ±¼)`; use `cvxopt.solvers.qp` or `scipy.optimize.minimize` for QP.

Week 138 â€” ESL Ch. 13â€“14: Prototype Methods & Unsupervised Learning
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Implement k-means, k-medoids, and Gaussian mixture models (GMM) via EM; understand learning vector quantization (LVQ); implement hierarchical clustering with different linkages; understand self-organizing maps (SOM); derive EM algorithm for GMM from first principles.
- âœ… Pass: Implement k-means from scratch and prove convergence (monotonic decrease of objective); implement k-medoids (PAM algorithm); derive EM algorithm for GMM (E-step: compute responsibilities, M-step: update parameters); implement GMM-EM and compare to k-means; implement hierarchical clustering with single, complete, and average linkage; compute cophenetic correlation; implement LVQ and compare to k-means; visualize dendrograms and cluster quality metrics (silhouette, Davies-Bouldin).
- ğŸ› ï¸ How: k-means: iterate assign-to-nearest-centroid, update-centroids; objective: `Î£áµ¢ Î£â‚– ráµ¢â‚–||xáµ¢-Î¼â‚–||Â²`; EM for GMM: `Î³áµ¢â‚– = Ï€â‚– N(xáµ¢|Î¼â‚–,Î£â‚–) / Î£â±¼ Ï€â±¼ N(xáµ¢|Î¼â±¼,Î£â±¼)`, update `Ï€â‚– = Î£Î³áµ¢â‚–/n`, `Î¼â‚– = Î£Î³áµ¢â‚–xáµ¢/Î£Î³áµ¢â‚–`, `Î£â‚– = Î£Î³áµ¢â‚–(xáµ¢-Î¼â‚–)(xáµ¢-Î¼â‚–)áµ€/Î£Î³áµ¢â‚–`; hierarchical: `scipy.cluster.hierarchy`; cophenetic: correlation between pairwise distances and dendrogram heights.

Week 139 â€” ESL Ch. 15: Random Forests
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Understand bagging and its variance reduction; implement random forests from scratch with bootstrap sampling and feature subsampling; compute out-of-bag (OOB) error as unbiased test error estimate; understand variable importance measures (permutation, Gini); analyze effect of correlation between trees.
- âœ… Pass: Implement random forest from scratch (bootstrap samples, random feature subset at each split, majority vote/averaging); compute OOB error and compare to test error and cross-validation; implement variable importance via permutation (OOB samples) and Gini decrease; demonstrate variance reduction compared to single tree via bias-variance decomposition; analyze effect of number of features sampled (mtry) on correlation between trees and forest performance; produce partial dependence plots.
- ğŸ› ï¸ How: Random forest: build B trees each on bootstrap sample with feature subsampling (âˆšp for classification, p/3 for regression); OOB error: for each observation, average predictions from trees not containing it in bootstrap sample; permutation importance: shuffle feature j in OOB data, compute increase in OOB error; Gini importance: sum Gini decrease when splitting on feature across all trees; correlation: compute pairwise correlation of tree predictions.

Week 140 â€” ESL Ch. 16â€“17: Ensemble Learning & Graphical Models
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Master ensemble methods theory; understand stacking and super learner; implement Bayesian model averaging; explore undirected graphical models (Markov networks); understand conditional independence and the Hammersley-Clifford theorem; implement graphical lasso for sparse inverse covariance estimation.
- âœ… Pass: Implement stacked generalization (train meta-learner on out-of-fold predictions); implement super learner with cross-validation-based weighting; compute Bayesian model averaging weights using BIC approximation; implement graphical lasso (L1-penalized precision matrix estimation) and visualize resulting network; test conditional independence using partial correlations; compare ensemble methods (bagging, boosting, stacking) on same dataset with â‰¥5 base learners; produce detailed analysis of why and when each ensemble method excels.
- ğŸ› ï¸ How: Stacking: train base learners, collect out-of-fold predictions, train meta-learner on these; super learner: non-negative weights minimizing CV error `min Î£(yáµ¢ - Î£Î±â‚–fÌ‚â‚–â½â»â±â¾)Â²` subject to `Î±â‰¥0`, `Î£Î±=1`; BMA weights: `w_k âˆ exp(-BIC_k/2)`; graphical lasso: `max log det Î˜ - tr(SÎ˜) - Î»||Î˜||â‚`; use `sklearn_glasso` or ADMM implementation; zero entries in Î˜ imply conditional independence.

Week 141 â€” ESL Ch. 18 & Integration: High-Dimensional Problems
- ğŸ“– [ESL](https://hastie.su.domains/ElemStatLearn/)
- ğŸ§ª Practice: Understand challenges in high-dimensional settings (p >> n); implement elastic net combining L1 and L2 penalties; understand the Lasso path and LARS algorithm; implement fused lasso for spatial/temporal smoothing; master multiple testing correction (FDR, FWER); understand compressed sensing and restricted isometry property.
- âœ… Pass: Implement elastic net and demonstrate scenarios where it outperforms pure lasso or ridge; implement LARS algorithm and verify equivalence to lasso path; implement fused lasso for 1D signal denoising; apply multiple testing corrections (Bonferroni, Holm, Benjamini-Hochberg) and compare false discovery rates via simulation; demonstrate compressed sensing recovery with RIP-satisfying matrices; integrate â‰¥5 ESL techniques in comprehensive analysis comparing interpretability, prediction accuracy, computational cost, and theoretical guarantees.
- ğŸ› ï¸ How: Elastic net: `min ||y-XÎ²||Â² + Î»â‚||Î²||â‚ + Î»â‚‚||Î²||Â²`; LARS: forward stagewise that adds most correlated predictor and moves in equiangular direction; fused lasso: `min ||y-Î²||Â² + Î»â‚||Î²||â‚ + Î»â‚‚Î£|Î²áµ¢-Î²áµ¢â‚Šâ‚|`; FDR: Benjamini-Hochberg procedure sorting p-values; compressed sensing: recover sparse signal from few measurements when sensing matrix satisfies RIP; compare convergence and solution paths.
</details>

ğŸ” Flex â€” Statistical learning theory consolidation

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 15 Â· Deep Learning â€” Weeks 158â€“177 (Complete D2L fundamentals, Goodfellow DL)</b></summary>

Weeks 158â€“165 â€” D2L (Fundamentals)
- ğŸ“– [D2L](https://d2l.ai)
- ğŸ§ª Practice: Topic-specific small models exactly as covered (MLP, CNN, RNN; optimization; regularization; data pipelines)
- âœ… Pass (weekly): Train the chapterâ€™s model variant on a toy dataset with fixed seeds and one controlled ablation (optimizer OR regularization) taught in D2L; log curves/metrics.
- ğŸ› ï¸ How: Follow D2Lâ€™s PyTorch/MXNet examples; fix seeds; keep experiments minimal and reproducible.

Week 166 â€” The Illustrated Transformer (Bridge)
- ğŸ“– [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- ğŸ§ª Practice: Understand the Transformer architecture: self-attention mechanism, multi-head attention, positional encoding, encoder-decoder structure.
- âœ… Pass: Implement self-attention from scratch; verify tensor shapes at each step; implement attention masking; write unit tests for: (1) output shape correctness, (2) masked positions get zero attention, (3) attention weights sum to 1.
- ğŸ› ï¸ How: Use NumPy or PyTorch; implement Q, K, V projections; scaled dot-product attention; verify with `assert` statements and test cases.

Weeks 167â€“177 â€” Deep Learning Book (Complete)
- ğŸ“– [Deep Learning Book](https://www.deeplearningbook.org/)
- ğŸ§ª Practice: For each chapter, run a small experiment that demonstrates the chapterâ€™s key concept using building blocks learned in D2L
- âœ… Pass (weekly): Provide a controlled comparison or demonstration plot showing the expected qualitative effect (e.g., different inits, L2 vs dropout, step-size schedules).
- ğŸ› ï¸ How: Small synthetic or standard toy datasets; fixed seeds; log and compare curves cleanly.
</details>

ğŸ” Flex â€” DL recap + tracked mini project

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 17 Â· LLMs & Agents â€” Weeks 202â€“205 (Complete HF Course + Agents)</b></summary>

Weeks 202â€“204 â€” Hugging Face Course
- ğŸ“– [HF Course](https://huggingface.co/course/chapter1)
- ğŸ§ª Practice: Learn the Hugging Face ecosystem: load and preprocess datasets, understand tokenizers, fine-tune pretrained models, run inference, evaluate with appropriate metrics.
- âœ… Pass (weekly): Complete the course exercises for that week's chapters; fine-tune a small transformer on a downstream task (e.g., text classification, NER); evaluate with task-appropriate metrics (accuracy, F1, etc.); log all configurations.
- ğŸ› ï¸ How: `transformers` library for models; `datasets` for data loading; `Trainer` API for fine-tuning; `accelerate` for distributed training; Weights & Biases or TensorBoard for logging.

Week 205 â€” HF Agents
- ğŸ“– [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)
- ğŸ§ª Practice: Build AI agents that use tools: understand agent architectures, implement tool calling, handle errors and timeouts, implement safety guardrails.
- âœ… Pass: Build an agent that completes a multi-step task using external tools; implement proper timeout handling; test with an injected failure scenario and verify graceful degradation; document safety checks and limitations.
- ğŸ› ï¸ How: Use Hugging Face agents framework; implement `Tool` classes; set timeouts with `asyncio.timeout` or similar; log all tool calls and responses; implement input validation.
</details>

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 13 Â· R for Data Science â€” Weeks 142â€“151 (Complete R4DS 2e)</b></summary>

Weeks 142â€“151 â€” R4DS (Complete)
- ğŸ“– [R for Data Science (2e)](https://r4ds.hadley.nz)
- ğŸ§ª Practice: Learn R and tidyverse progressively: data import, tidying (pivot_longer/wider), transformation (dplyr verbs), visualization (ggplot2), strings, factors, dates, functions, iteration, and communication (Quarto/RMarkdown).
- âœ… Pass (weekly): Complete a mini-analysis using only functions from chapters covered that week; produce a Quarto/RMarkdown report that renders end-to-end; include at least one visualization and one summary table.
- ğŸ› ï¸ How: `library(tidyverse)`; `read_csv`; `dplyr` verbs (`filter`, `mutate`, `summarize`, `group_by`); `ggplot2`; `set.seed()` for reproducibility.
</details>

ğŸ” Flex â€” R consolidation

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 9 Â· Econometrics & Time Series â€” Weeks 90â€“111 (Complete Gujarati, LÃ¼tkepohl)</b></summary>

Weeks 90â€“101 â€” Basic Econometrics (complete)
- ğŸ“– [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- ğŸ§ª Practice: Reproduce a worked example per chapter using methods from that chapter only (OLS basics; classical assumption diagnostics; heteroskedasticity/autocorrelation remedies; functional form; limited dependent variables as presented)
- âœ… Pass (weekly): Match the textbook exampleâ€™s coefficients and standard errors (within rounding) and include one robustness check discussed in that chapter (e.g., robust/HAC SEs when appropriate).
- ğŸ› ï¸ How: `statsmodels` OLS/GLM, `cov_type="HC3"` or HAC if the chapter addresses it; include diagnostic plots taught there.

Weeks 102â€“111 â€” LÃ¼tkepohl (complete)
- ğŸ“– [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- ğŸ§ª Practice: Implement multivariate time series analysis: VAR model specification, estimation, lag order selection, stability analysis, impulse response functions, forecast error variance decomposition, and cointegration/VECM.
- âœ… Pass (weekly): Fit VAR/VECM to macroeconomic data; select lag order using information criteria; verify stability (roots inside unit circle); compute and plot IRFs with confidence bands; perform Johansen cointegration test when applicable.
- ğŸ› ï¸ How: `statsmodels.tsa.api.VAR`; `statsmodels.tsa.vector_ar.vecm.VECM`; `irf()` for impulse responses; rolling-window forecasts for evaluation.
</details>

ğŸ” Flex â€” Econometrics/time-series consolidation

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 10 Â· Causal Inference â€” Weeks 112â€“121 (Complete The Mixtape)</b></summary>

Week 112 â€” Properties of Regression, DAGs, Potential Outcomes
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Understand Simpson's paradox and collider bias; draw and analyze directed acyclic graphs (DAGs); master potential outcomes framework; understand Average Treatment Effect (ATE) and selection bias.
- âœ… Pass: Implement Simpson's paradox example showing reversal of association; construct â‰¥3 DAGs identifying confounders, mediators, and colliders; derive ATE under different selection mechanisms; demonstrate selection bias analytically and via simulation.
- ğŸ› ï¸ How: Use `networkx` or `dagitty` for DAG visualization; simulate counterfactuals with fixed treatment assignments; compute `E[YÂ¹] - E[Yâ°]` vs observed difference-in-means; show bias = `E[Yâ°|D=1] - E[Yâ°|D=0]`.

Week 113 â€” Randomized Controlled Trials & Matching
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Understand randomization inference; implement exact matching, propensity score matching (PSM), and coarsened exact matching; check covariate balance; assess common support.
- âœ… Pass: Analyze an RCT dataset computing ATE with randomization inference (permutation test); implement PSM with â‰¥3 matching algorithms (nearest neighbor, caliper, kernel); produce balance tables and Love plots before/after matching; check common support with density plots; report treatment effects with bootstrapped standard errors.
- ğŸ› ï¸ How: Permutation test: shuffle treatment vector 1000+ times, recompute difference-in-means; `sklearn.neighbors.NearestNeighbors` for matching; logistic regression for propensity scores; standardized mean differences for balance; `seaborn.kdeplot` for common support.

Week 114 â€” Regression Discontinuity Design (RDD)
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Understand sharp and fuzzy RDD; check continuity assumptions; select bandwidth using cross-validation and optimal methods; test for manipulation of running variable; implement local polynomial regression.
- âœ… Pass: Apply RDD to real or simulated data with a known cutoff; test for discontinuity at the threshold using local linear regression with â‰¥3 bandwidths; perform McCrary density test for manipulation; produce RDD plots showing outcome vs running variable with fitted lines; report local average treatment effect (LATE) with robust standard errors; conduct placebo tests at false cutoffs.
- ğŸ› ï¸ How: Local linear regression within bandwidth h: `Y ~ D + (X-c) + D*(X-c)` for |X-c| < h; optimal bandwidth via `rdrobust` (R) or manual cross-validation; McCrary test: fit separate densities left/right of cutoff and test for jump; bootstrap for inference.

Week 115 â€” Instrumental Variables (IV)
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Understand endogeneity and IV identification; implement two-stage least squares (2SLS); test instrument relevance and exogeneity; understand weak instruments problem; compute local average treatment effect (LATE) with compliance types.
- âœ… Pass: Identify a valid instrument and justify exclusion restriction; implement 2SLS manually (first stage, second stage) and compare to built-in IV estimator; test instrument strength (F-stat > 10 rule of thumb, Cragg-Donald); perform overidentification test when multiple instruments available; compute LATE and interpret in terms of compliers; conduct sensitivity analysis for violation of exclusion restriction.
- ğŸ› ï¸ How: Manual 2SLS: regress X on Z (first stage), predict XÌ‚, regress Y on XÌ‚ (second stage); `statsmodels.sandbox.regression.gmm.IV2SLS` or `linearmodels.iv.IV2SLS`; first-stage F-stat for relevance; Hansen J-stat for overidentification; bound analysis for exclusion restriction violations.

Week 116 â€” Panel Data & Fixed Effects
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Understand within-group variation; implement fixed effects (FE) and first differences (FD); test fixed vs random effects (Hausman test); handle time-varying treatments; understand parallel trends assumption.
- âœ… Pass: Estimate panel data model with entity and time fixed effects; compare pooled OLS, FE, and random effects; perform Hausman test; demean data manually and verify equivalence to FE estimator; produce event study plots for dynamic treatment effects; test parallel trends visually and formally; cluster standard errors at appropriate level.
- ğŸ› ï¸ How: FE via demeaning: `Y_it - È²_i = (X_it - XÌ„_i)Î² + (Îµ_it - ÎµÌ„_i)`; `linearmodels.panel.PanelOLS` with `entity_effects=True`; Hausman test compares FE vs RE; event study: include leads/lags of treatment; plot coefficients with 95% CIs; cluster SEs: `cov_type='clustered'`.

Week 117 â€” Difference-in-Differences (DiD)
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Implement canonical 2Ã—2 DiD; test parallel trends assumption; handle staggered treatment adoption; understand two-way fixed effects (TWFE) issues with heterogeneous treatment effects; apply robust DiD estimators.
- âœ… Pass: Estimate 2Ã—2 DiD with interaction term and verify equivalence to group-time means; test parallel trends with pre-treatment period placebo tests; visualize trends with event study; implement staggered DiD using TWFE and compare to Callaway-Sant'Anna or Sun-Abraham estimators to avoid bias from heterogeneous effects; report treatment effects with wild cluster bootstrap standard errors.
- ğŸ› ï¸ How: DiD: `Y = Î²â‚€ + Î²â‚Â·Treated + Î²â‚‚Â·Post + Î²â‚ƒÂ·(TreatedÃ—Post)`; parallel trends: plot group-specific trends pre-treatment; placebo DiD on earlier periods; for staggered adoption, never-treated as control group; decompose TWFE weights; wild bootstrap: `clustered_bootstrap` with Rademacher weights.

Week 118 â€” Synthetic Control Method
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Understand synthetic control as data-driven matching on pre-treatment outcomes; implement synthetic control optimization; conduct permutation-based inference; assess fit quality; handle multiple treated units.
- âœ… Pass: Apply synthetic control to a policy intervention; construct synthetic control by optimizing weights on donor pool to match pre-treatment outcomes; report weights and predictor balance; visualize treated vs synthetic trends; conduct placebo tests by reassigning treatment to each donor; compute p-values from permutation distribution; assess robustness by excluding donors iteratively; report pre/post-treatment RMSPE ratio.
- ğŸ› ï¸ How: Synthetic control: minimize `||Xâ‚ - Xâ‚€W||` subject to `W â‰¥ 0`, `âˆ‘W = 1`, where Xâ‚ is treated unit pre-treatment outcomes, Xâ‚€ is donor matrix; use `scipy.optimize.minimize` with constraints or quadratic programming; permutation inference: apply method to each control unit, rank treatment effect; gap plot showing treated - synthetic over time; leave-one-out for robustness.

Week 119 â€” Regression Kink Design & Bunching
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Understand regression kink design (RKD) as derivative discontinuity; implement bunching estimator for detecting behavioral responses; test for slope changes; estimate elasticities.
- âœ… Pass: Apply RKD to a policy with kinked schedule (e.g., tax, subsidy); test for change in slope at kink point using local polynomial regression on subsamples; visualize kink with binned scatter plot; implement bunching estimator by comparing empirical distribution to counterfactual; estimate excess mass and implied elasticity; conduct robustness checks varying excluded region and polynomial order.
- ğŸ› ï¸ How: RKD: estimate `dY/dX` separately left/right of kink, test equality; local linear separately each side: `Y ~ (X-k) + covariates` for X near k; binned scatter: equal-sized bins, plot means; bunching: integrate empirical density, fit counterfactual excluding region around kink (polynomial fit), excess mass = observed - counterfactual; elasticity from excess mass and tax change.

Week 120 â€” Regression Sensitivity & Bounds
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Assess robustness using omitted variable bias (OVB) formulas; implement Oster (2019) bounds; conduct sensitivity analysis for unobserved confounding; use Rosenbaum bounds for matching estimators; understand partial identification.
- âœ… Pass: Apply OVB formula to show direction/magnitude of bias from omitted confounder; implement Oster method computing Î´ (relative importance of unobservables) for null result; produce sensitivity plots showing treatment effect as function of confounder strength; apply Rosenbaum bounds to PSM estimates varying Î“; report identified set and discuss assumption needed for causal claim; compare naÃ¯ve, conditional, and bounded estimates.
- ğŸ› ï¸ How: OVB: `Î²Ì‚ = Î² + Î³Â·Î´` where Î³ is effect of omitted U on Y, Î´ is coefficient from X ~ U; Oster Î´: `Î´ = [RÂ²max - RÌƒÂ²]/[RÌƒÂ² - RÂ°Â²] Â· [Î²Ìƒ - Î²*]/[Î²Â° - Î²Ìƒ]`; plot treatment effect vs confounding strength; Rosenbaum Î“: recompute p-value under assumption of hidden bias; identified set: report range of treatment effects consistent with assumptions.

Week 121 â€” Advanced Topics & Review
- ğŸ“– [The Mixtape](https://mixtape.scunning.com)
- ğŸ§ª Practice: Integrate multiple identification strategies; understand machine learning for causal inference (double/debiased ML, causal forests); review all methods; conduct sensitivity analysis across multiple methods.
- âœ… Pass: Apply â‰¥3 causal methods to the same research question; compare point estimates and confidence intervals; discuss relative credibility of each design; implement double ML for treatment effect estimation in high-dimensional setting; report model-averaged treatment effects and conduct multi-method sensitivity analysis; produce comprehensive writeup documenting identification assumptions, threats to validity, and robustness.
- ğŸ› ï¸ How: Compare DiD, IV, RDD on same outcome; assess common support, parallel trends, instrument strength; double ML: use cross-fitting with `DoubleMLPLR` or manual implementation (Lasso for Y~X, D~X, residualize); causal forest: `grf` package (R) or `econml.dml.CausalForestDML` (Python); plot distribution of treatment effects; report heterogeneity by subgroups; synthesis table with all estimates.
</details>

ğŸ” Flex â€” Causal inference consolidation

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 16 Â· MLOps & Data Engineering â€” Weeks 178â€“201 (Complete Zoomcamps, ML Systems)</b></summary>

Weeks 178â€“185 â€” MLOps Zoomcamp
- ğŸ“– [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- ğŸ§ª Practice: Module-by-module implementation as taught (tracking, packaging, CI, serving, orchestration, monitoring)
- âœ… Pass (weekly): A runnable local pipeline from clean state to served endpoint with tests passing for that weekâ€™s scope.
- ğŸ› ï¸ How: Docker/Compose; MLflow/W&B; `pytest`; minimal infra defined as per module.

Weeks 186â€“193 â€” Machine Learning Systems
- ğŸ“– [ML Systems](https://mlsysbook.ai)
- ğŸ§ª Practice: Write/extend a system design doc each week focusing only on that weekâ€™s concepts (SLA/SLOs; rollout/rollback; monitoring; data contracts; cost/reliability)
- âœ… Pass (weekly): The doc includes concrete metrics, failure scenarios, and operational procedures aligned to the chapter.
- ğŸ› ï¸ How: ADR template; simple diagrams-as-code optional (e.g., Mermaid).

Weeks 194â€“201 â€” Data Engineering Zoomcamp
- ğŸ“– [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- ğŸ§ª Practice: Module-by-module pipeline work (ingestion, storage, batch/stream, orchestration, analytics eng, testing) as taught in the course
- âœ… Pass (weekly): Re-deployable pipeline from scratch with idempotent runs for that moduleâ€™s scope.
- ğŸ› ï¸ How: Terraform/Docker where required, dbt, Airflow/Prefect according to the module.
</details>

ğŸ” Flex â€” Ops/engineering consolidation

---------------------------------------------------------------------

---------------------------------------------------------------------

<details>
<summary><b>Phase 18 Â· Consolidation, Capstone, Portfolio â€” Weeks 206â€“209</b></summary>

Week 206 â€” statsmodels deep dive
- ğŸ“– [statsmodels](https://www.statsmodels.org/stable/index.html)
- ğŸ§ª Practice: Master statsmodels by reproducing analyses from earlier phases: OLS with diagnostics, GLMs, time series models (ARIMA, VAR), hypothesis testing.
- âœ… Pass: Reproduce two econometric analyses matching original coefficients and standard errors; include full diagnostic suite (heteroskedasticity, autocorrelation tests); apply robust SEs where violations exist.
- ğŸ› ï¸ How: `statsmodels.api.OLS/GLM`; `statsmodels.tsa` for time series; `het_breuschpagan`, `acorr_ljungbox` for diagnostics; `cov_type="HC3"` for robust SEs.

Week 207 â€” scikit-learn deep dive
- ğŸ“– [scikit-learn](https://scikit-learn.org/stable/index.html)
- ğŸ§ª Practice: Create a production-ready ML pipeline template: preprocessing (scaling, encoding), feature selection, model training with CV, hyperparameter tuning, probability calibration.
- âœ… Pass: Build a complete Pipeline with ColumnTransformer for mixed types; implement nested CV for unbiased evaluation; apply probability calibration (Platt scaling or isotonic); ensure deterministic results with fixed seeds.
- ğŸ› ï¸ How: `Pipeline`; `ColumnTransformer`; `GridSearchCV`/`RandomizedSearchCV`; `CalibratedClassifierCV`; fixed `random_state` throughout.

Weeks 208â€“209 â€” Capstone & Portfolio
- ğŸ“– Integrate end-to-end skills only from prior phases
- ğŸ§ª Practice: Complete a capstone project demonstrating: problem framing, data pipeline, modeling with uncertainty quantification, model interpretation, rigorous evaluation, and stakeholder communication.
- âœ… Pass: Deliver a fully reproducible project (single command to run); include README documenting problem, approach, assumptions, limitations, and risks; provide model interpretation (SHAP/PDP); write a 1-page non-technical summary for stakeholders.
- ğŸ› ï¸ How: Use Git for version control; Docker for reproducibility; include uncertainty estimates (bootstrap CIs or Bayesian); create visualizations for non-technical audience; document all decisions.
</details>

---------------------------------------------------------------------


---------------------------------------------------------------------

Resource-to-Week Completion Map (cover-to-cover)
- Python for Data Analysis â€” Weeks 1â€“8 â€” [Python for Data Analysis](https://wesmckinney.com/book/)
- Mathematics for Machine Learning â€” Weeks 9â€“18 â€” [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- Think Stats â€” Weeks 19â€“24 â€” [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- PSU STAT 505 (Applied Multivariate Statistics) â€” Weeks 25â€“39 â€” [PSU STAT 505](https://online.stat.psu.edu/stat505)
- Think Bayes â€” Weeks 40â€“47 â€” [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- Flexible Imputation of Missing Data â€” Weeks 48â€“51 â€” [FIMD](https://stefvanbuuren.name/fimd/)
- ISLP (Statistical Learning with Python) â€” Weeks 52â€“61 â€” [ISLP](https://www.statlearning.com/)
- PRML (Bishop) â€” Weeks 62â€“75 â€” [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- Interpretable Machine Learning â€” Weeks 76â€“80 â€” [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- Data Mining: Concepts and Techniques (3e) â€” Weeks 81â€“89 â€” [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- Basic Econometrics (Gujarati) â€” Weeks 90â€“101 â€” [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- New Introduction to Multiple Time Series (LÃ¼tkepohl) â€” Weeks 102â€“111 â€” [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- Causal Inference: The Mixtape (Cunningham) â€” Weeks 112â€“121 â€” [The Mixtape](https://mixtape.scunning.com)
- Convex Optimization (Boyd & Vandenberghe) â€” Weeks 122â€“131 â€” [Convex Optimization (PDF)](https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- The Elements of Statistical Learning (Hastie, Tibshirani, Friedman) â€” Weeks 132â€“141 â€” [ESL](https://hastie.su.domains/ElemStatLearn/)
- R for Data Science (2e) â€” Weeks 142â€“151 â€” [R for Data Science (2e)](https://r4ds.hadley.nz)
- Beautiful Soup â€” Week 152 â€” [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- Selenium (Python) â€” Weeks 153â€“154 â€” [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- SQL Roadmap (GeeksforGeeks) â€” Weeks 155â€“157 â€” [SQL Roadmap](https://www.geeksforgeeks.org/blogs/sql-roadmap/)
- Dive into Deep Learning â€” Weeks 158â€“165 â€” [D2L](https://d2l.ai)
- The Illustrated Transformer â€” Week 166 â€” [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- Deep Learning â€” Weeks 167â€“177 â€” [Deep Learning Book](https://www.deeplearningbook.org/)
- MLOps Zoomcamp â€” Weeks 178â€“185 â€” [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- Machine Learning Systems â€” Weeks 186â€“193 â€” [ML Systems](https://mlsysbook.ai)
- Data Engineering Zoomcamp â€” Weeks 194â€“201 â€” [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- HF Course + HF Agents â€” Weeks 202â€“205 â€” [HF Course](https://huggingface.co/course/chapter1), [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)

Notes
- Keep work in any format; seed randomness for reproducibility.
- Use Flex Weeks to finish pass items, review tricky parts, and add spaced-repetition cards (optional).

**Important Limitation Regarding Theoretical Econometrics**

This roadmap includes foundational econometric resources (Gujarati for Basic Econometrics, LÃ¼tkepohl for Time Series, Cunningham for Causal Inference) that provide essential intuition and practical implementation skills. However, **these resources have limited theoretical rigor and depth compared to advanced theoretical econometrics textbooks** such as:
- Greene's *Econometric Analysis*
- Hayashi's *Econometrics*  
- Hansen's *Econometrics*
- Hamilton's *Time Series Analysis*
- Wooldridge's graduate texts

The included materials **do not cover** advanced topics with mathematical rigor:
- Asymptotic theory (consistency, asymptotic normality, delta method)
- Formal proofs of estimator properties
- Generalized Method of Moments (GMM) with complete derivations
- Maximum likelihood theory and proofs
- Rigorous treatment of weak instruments, specification tests, and identification
- Advanced time series theory (unit root asymptotics, cointegration proofs)

**If your goal is theoretical mastery of econometrics** (e.g., PhD-level econometrics, original methodological research, or formal proofs), you should supplement this roadmap with the advanced theoretical texts listed above or pursue a dedicated econometrics curriculum. The current roadmap emphasizes **applied intuition and practical implementation** over formal mathematical rigor in econometric theory.

- Back to top â†‘

# ğŸš€ AI & Data Science Roadmap

A rigorously structured, end-to-end learning path for everyone starting from zero: absolute beginners, career switchers, students, and working professionals studying partâ€‘time. The aim is to graduate as a Data Science Expert grounded in descriptive and inferential statistics, mathematics, econometrics and causal inference, classical ML, deep learning, and production practices.

## Contents
- [How to use this roadmap](#how-to-use-this-roadmap)
- [Pacing model (time assumptions)](#pacing-model-time-assumptions)
- [Competency outcomes (what â€œData Science Expertâ€ means)](#competency-outcomes-what-data-science-expert-means)
- [Core Curriculum (Stages 0â€“10)](#core-curriculum)
  - [Stage 0 â€” Orientation and Study Setup](#stage-0-orientation-and-study-setup)
  - [Stage 1 â€” Mathematics for ML and Optimization](#stage-1-mathematics-for-ml-and-optimization)
  - [Stage 2 â€” Probability and Statistics](#stage-2-probability-and-statistics)
  - [Stage 3 â€” Programming Fundamentals (Python-first)](#stage-3-programming-fundamentals-python-first)
  - [Stage 4 â€” EDA and Visualization](#stage-4-eda-and-visualization)
  - [Stage 5 â€” SQL and Data Modeling](#stage-5-sql-and-data-modeling)
  - [Stage 6 â€” Data Acquisition: Web Scraping and APIs](#stage-6-data-acquisition-web-scraping-and-apis)
  - [Stage 7 â€” Econometrics, Causal Inference, and Time Series](#stage-7-econometrics-causal-inference-and-time-series)
  - [Stage 8 â€” Classical Machine Learning](#stage-8-classical-machine-learning)
  - [Stage 9 â€” Deep Learning](#stage-9-deep-learning)
  - [Stage 10 â€” MLOps and Data Engineering](#stage-10-mlops-and-data-engineering)
- [Specialization Tracks (optional)](#specialization-tracks-optional)
  - [NLP and LLMs](#b-nlp-and-llms)
  - [Computer Vision](#c-computer-vision)
  - [Recommender Systems](#d-recommender-systems)
- [Capstone and Portfolio](#capstone-and-portfolio)
- [Appendix](#appendix)

---

## How to use this roadmap

- Follow stages in order; specialize after Stage 10 or alongside it if time allows.
- Each stage includes: Objectives, Place in our goal, Prerequisites, Estimated Effort, Weekly progression (subâ€‘stages), Essential/Supplementary resources with â€œWhy nowâ€, Practice aligned to what you just learned, Exit Criteria.
- Short, targeted onâ€‘ramps are provided for true beginners. These are practical and scoped to avoid fatigue.
- Evidence-based progress: Treat each week as a study cycle. Produce the listed deliverable, check Exit Criteria, and keep a learning log (what you read, built, measured).

---

## Pacing model (time assumptions)

- For everyone: beginners, switchers, students, and working professionals.
- Estimates are net study hours per week:
  - Light pace: 3â€“5 h/week
  - Standard pace: 5â€“7 h/week (recommended)
  - Intensive pace: 8â€“12 h/week

Methodology for estimates:
- Textbooks/notes ~8â€“12 pages/hour + similar time for exercises/notes.
- Videos: runtime Ã— 1.4â€“1.7 (pauses + noteâ€‘taking + small practice).
- Docs/tutorials: quickstarts 1â€“3 h; deeper guides 4â€“8 h.
- Projects are included where listed (typically 2â€“6 h).

Tip: If time is tight, complete the weekly Practice (Deliverable) to keep momentum; return to deeper reading next week.

---

## Competency outcomes (what â€œData Science Expertâ€ means)

By the end, you will be able to:
- Frame problems, design datasets/pipelines, and select appropriate statistical/ML methods.
- Apply descriptive and inferential statistics correctly, quantify uncertainty, and design credible experiments.
- Use econometrics and causal inference to estimate effects under assumptions; analyze panel and timeâ€‘series data with proper diagnostics and backtesting.
- Build and evaluate classical ML and deep learning models; interpret, communicate, and document decisions.
- Acquire data via APIs/scraping responsibly; transform with SQL; manage data quality; visualize and narrate insights.
- Ship reproducible projects with tests, containers, experiment tracking, CI/CD, and basic orchestration.

---

<a id="core-curriculum"></a>
## Core Curriculum (Stages 0â€“10)

<a id="stage-0-orientation-and-study-setup"></a>
### Stage 0 â€” Orientation and Study Setup
Estimated effort (total): 6â€“10 h Â· Calendar: ~1â€“2 weeks at 3â€“7 h/week

- Place in our goal: Establishes foundational habits (version control, environments) that make all subsequent learning reproducible and professional.
- Objectives: Adopt effective study habits; set up dev environment.
- Prerequisites: None

Zeroâ€‘toâ€‘One Onâ€‘Ramp (optional)
- The Missing Semester (Shell/CLI essentials, 4â€“6 h skim) â€” [The Missing Semester](https://missing.csail.mit.edu/) â€” Why now: Commandâ€‘line fluency accelerates all later work.

Weekly progression
- Week 0.1 (3â€“5 h) â€” [Using venv](https://docs.python.org/3/library/venv.html) â€” Why now: Clean, reproducible environments from day one.  
  Practice (Deliverable): Create and activate a venv; freeze requirements; add setup.md.

  
- Week 0.2 (3â€“5 h) â€” [Set up Git](https://docs.github.com/en/get-started/quickstart/set-up-git) â€” Why now: Version control and collaboration are foundational.  
  Practice (Deliverable): Initialize a repo; commit a template project; open a practice PR and merge it.

Supplementary
- [Poetry](https://python-poetry.org/docs/) or [Conda](https://docs.conda.io/en/latest/) â€” Optional environment managers.
- [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/) â€” Project scaffolding.

Exit Criteria
- You can manage Python environments and maintain a clean repository.

---

<a id="stage-1-mathematics-for-ml-and-optimization"></a>
### Stage 1 â€” Mathematics for ML and Optimization
Estimated effort (total): 62â€“96 h Â· Calendar: ~9â€“16 weeks at 5â€“7 h/week

- Place in our goal: Mathematical language and optimization tools powering ML/DL training, diagnostics, and interpretation.
- Objectives: Linear algebra, vector calculus, optimization basics; probability primer.
- Prerequisites: None (onâ€‘ramp below if needed)

Zeroâ€‘toâ€‘One Onâ€‘Ramp (short; pick A or B)
- A) Algebra/Trig quick review (2â€“6 h) â€” Complete: Algebra Review Sections 1â€“5; Trig up to Unit Circle  
  [Paulâ€™s Algebra/Trig Review](https://tutorial.math.lamar.edu/Extras/AlgebraTrigReview/AlgebraTrigIntro.aspx) â€” Why now: Concise refresh without a full course.
- B) Linear algebra intuition (3â€“6 h) â€” Complete: Episodes 1, 2, 3, 4, 6, and 8  
  [3Blue1Brown â€” Essence of Linear Algebra](https://www.3blue1brown.com/topics/linear-algebra) â€” Why now: Geometric intuition accelerates learning.

Weekly progression
- Week 1 (8â€“10 h) â€” MML Ch. 2 Linear Algebra (vectors, matrices, operations)  
  Resource: [Mathematics for Machine Learning](https://mml-book.github.io/) â€” Why now: LA underpins ML representations.  
  Practice (Deliverable): Compute dot products, norms, projections; solve 2Ã—2/3Ã—3 Ax=b (by hand and in NumPy); verify residuals < 1eâ€‘8.

  
- Week 2 (6â€“8 h) â€” MML Ch. 3 Analytic Geometry (subspaces, orthogonality)  
  Practice (Deliverable): Decompose vectors into parallel/orthogonal components; leastâ€‘squares line fit via normal equations; residual plot.

  
- Week 3â€“4 (10â€“14 h) â€” MML Ch. 4 Matrix Decompositions (LU, QR, eigen)  
  Practice (Deliverable): Solve Ax=b with LU; leastâ€‘squares via QR; eigenâ€‘decompose a small symmetric matrix; power iteration convergence plot.  
  Reference (no practice dependency): [The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) â€” derivatives section.

  
- Week 5 (6â€“8 h) â€” MML Ch. 5 Vector Calculus (gradients, Jacobians)  
  Practice (Deliverable): Derive gradients for quadratic forms; verify with numerical gradients; short comparison notes.

  
- Week 6â€“7 (12â€“18 h) â€” Convex Optimization (Boyd & Vandenberghe)  
  Resource: [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) â€” Complete: Ch. 2, 3, 4, 9 (skip advanced proofs first pass).  
  Practice (Deliverable): Implement GD vs. momentum on convex quadratics; plot convergence; report iterations to tolerance.

  
- Week 8 (6â€“8 h) â€” Probability Primer  
  Resource: [OpenIntro Statistics](https://www.openintro.org/book/os/) â€” Complete: Ch. 3 (3.1â€“3.5) and Ch. 4 (4.1â€“4.3).  
  Practice (Deliverable): Simulate Bernoulli/Binomial/Normal; visualize LLN/CLT with code and commentary.

Supplementary
- [Trig Cheat Sheet](https://tutorial.math.lamar.edu/pdf/Trig_Cheat_Sheet.pdf) â€” Handy reference.
- Think Complexity (2e) â€” Allen Downey â€” [Book page](https://greenteapress.com/wp/think-complexity/) Â· [PDF](https://greenteapress.com/thinkcomplexity2/thinkcomplexity2.pdf)

Exit Criteria
- Comfortable with vectors/matrices, derivatives/gradients, and basic convexity; can solve Ax=b and implement GD with empirical convergence.

---

<a id="stage-2-probability-and-statistics"></a>
### Stage 2 â€” Probability and Statistics
Estimated effort (total): 70â€“120 h Â· Calendar: ~12â€“21 weeks at 5â€“7 h/week

- Place in our goal: Descriptive and inferential statistics for valid analysis, experimentation, and uncertainty quantification.
- Objectives: Probability, estimation, hypothesis testing, regression/ANOVA, Bayesian basics, experimental design.
- Prerequisites: Stage 1

Zeroâ€‘toâ€‘One Onâ€‘Ramp (short)
- StatQuest (6â€“12 h skim) â€” Complete: Descriptive, Probability Basics, Pâ€‘values/CI  
  [StatQuest](https://www.youtube.com/c/joshstarmer) â€” Why now: Clear intuition helps formal courses.

Weekly progression
- Week 1â€“2 (12â€“16 h) â€” Descriptive Statistics and Intro  
  Resource: [OpenIntro Statistics](https://www.openintro.org/book/os/) â€” Ch. 1â€“2.  
  Practice (Deliverable): Summary report with visuals (central tendency, spread, outliers) using a public dataset.

  
- Week 3â€“4 (15â€“25 h) â€” Probability Theory  
  Resource: [STAT 414](https://online.stat.psu.edu/stat414/) â€” Lessons 1â€“10 (core).  
  Practice (Deliverable): Monte Carlo sims (binomial, normal, exponential) with convergence plots and short writeâ€‘up.

  
- Week 5â€“7 (15â€“25 h) â€” Mathematical Statistics  
  Resource: [STAT 415](https://online.stat.psu.edu/stat415/) â€” Lessons 1â€“9 (estimation/inference).  
  Practice (Deliverable): Bootstrap CI vs. analytical CI on real data; compare coverage.

  
- Week 8â€“9 (12â€“16 h) â€” Regression and ANOVA  
  Resources: [STAT 501](https://online.stat.psu.edu/stat501/) Lessons 1â€“8; [STAT 502](https://online.stat.psu.edu/stat502/) Lessons 1â€“6.  
  Practice (Deliverable): Fit OLS; diagnostics (residuals, VIF); one/twoâ€‘way ANOVA with effect sizes.

  
- Week 10 (6â€“10 h) â€” Bayesian Primer  
  Resource: [Think Bayes (2e)](https://allendowney.github.io/ThinkBayes2/) â€” Ch. 1â€“4.  
  Practice (Deliverable): Betaâ€‘Binomial A/B; posterior predictive checks; compare to frequentist test.

  
- Week 11 (6â€“8 h) â€” Experimentation  
  Resources: A/B Testing [guide](https://vkteam.medium.com/practitioners-guide-to-statistical-tests-ed2d580ef04f#1e3b), [planning](https://towardsdatascience.com/step-by-step-for-planning-an-a-b-test-ef3c93143c0b).  
  Practice (Deliverable): Power analysis; preâ€‘registration; mock A/B analysis with a decision memo.

Supplementary
- STAT 484/485 (R): [course pages](https://online.stat.psu.edu/stat484-485/) â€” Alternative R path.

Exit Criteria
- Can design experiments, analyze results, and interpret regression models with quantified uncertainty.

---

<a id="stage-3-programming-fundamentals-python-first"></a>
### Stage 3 â€” Programming Fundamentals (Python-first)
Estimated effort (total): 20â€“36 h Â· Calendar: ~3â€“7 weeks at 5â€“7 h/week

- Place in our goal: Engineering practices for reliable, testable data projectsâ€”skills hiring managers expect.
- Objectives: Python fluency, packaging, testing, typing, notebook hygiene, core DS libs.
- Prerequisites: Stage 0â€“2

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- Automate the Boring Stuff (10â€“16 h skim) â€” Parts Iâ€“II  
  [Automate the Boring Stuff](https://automatetheboringstuff.com/)

Weekly progression
- Week 1 (6â€“10 h) â€” Python Basics  
  Resource: [Official Tutorial](https://docs.python.org/3/tutorial/) or [Python Crash Course](https://www.youtube.com/watch?v=rfscVS0vtbw).  
  Practice (Deliverable): CSV summary CLI with argparse, logging, and `--help`.

  
- Week 2 (6â€“10 h) â€” pandas/numpy  
  Resource: [Python for Data Analysis](https://wesmckinney.com/book/) â€” Indexing, GroupBy, Reshaping, Time Series.  
  Practice (Deliverable): Reusable cleaning script; benchmark vectorized vs. loops; short results table.

  
- Week 3 (6â€“10 h) â€” Packaging/testing/typing  
  Resources: [PEP 8](https://peps.python.org/pep-0008/), [pytest](https://docs.pytest.org/).  
  Practice (Deliverable): Package the cleaning script; add unit tests and type hints; build a wheel.

  
- Week 4 (2â€“6 h) â€” Statistical modeling intro  
  Resource: [statsmodels](https://www.statsmodels.org/stable/index.html) â€” OLS tutorial + GLM overview.  
  Practice (Deliverable): Fit OLS; produce a regression report with assumptions and limitations.

Supplementary
- Kevin Sheppard notes â€” [Python Notes (PDF)](https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2021.pdf)
- Real Python best practices â€” [Collection](https://realpython.com/tutorials/best-practices/)

Exit Criteria
- Comfortable with pandas, numpy, plotting; can ship a small, tested project.

---

<a id="stage-4-eda-and-visualization"></a>
### Stage 4 â€” EDA and Visualization
Estimated effort (total): 15â€“24 h Â· Calendar: ~2â€“5 weeks at 5â€“7 h/week

- Place in our goal: Turn raw data into insight; build communication skills and data quality discipline.
- Objectives: Data cleaning, profiling, visualization, data quality checks.
- Prerequisites: Stage 3

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- Kaggle Data Cleaning (3â€“5 h) â€” [Kaggle Data Cleaning](https://www.kaggle.com/learn/data-cleaning)

Weekly progression
- Week 1 (6â€“10 h) â€” Profiling and Cleaning  
  Resources: [ydataâ€‘profiling](https://ydata-profiling.ydata.ai/docs/master/), [Python for Data Analysis](https://wesmckinney.com/book/).  
  Practice (Deliverable): Profiling report; reproducible cleaning notebook + script; checklist of issues.

  
- Week 2 (6â€“10 h) â€” Visualization  
  Resources: [Seaborn](https://seaborn.pydata.org/), [Matplotlib](https://matplotlib.org/stable/).  
  Practice (Deliverable): Small EDA dashboard (static/light interactive) with 3â€“5 key charts.

  
- Week 3 (3â€“4 h) â€” Data Quality Tests  
  Resources: [Great Expectations](https://docs.greatexpectations.io/), [missingno](https://github.com/ResidentMario/missingno).  
  Practice (Deliverable): Expectation suites for key tables; CI job to run them.

Supplementary
- [Altair](https://altair-viz.github.io/)
- [Plotly](https://plotly.com/python/)

Exit Criteria
- You can profile, clean, visualize data, and communicate insights clearly.

---

<a id="stage-5-sql-and-data-modeling"></a>
### Stage 5 â€” SQL and Data Modeling
Estimated effort (total): 15â€“28 h Â· Calendar: ~2â€“5 weeks at 5â€“7 h/week

- Place in our goal: Efficient data extraction/joins and schema design for reliable analytics.
- Objectives: Querying, joins, window functions, indexes, query plans; basic modeling.
- Prerequisites: Stage 3â€“4

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- Khan Academy SQL (4â€“8 h skim) â€” [Khan Academy SQL](https://www.khanacademy.org/computing/computer-programming/sql)

Weekly progression
- Week 1 (6â€“10 h) â€” SQL Fundamentals  
  Resource: [SQL Tutorial](https://www.sqltutorial.org/).  
  Practice (Deliverable): CRUD + analytical joins/subqueries; include result screenshots.

  
- Week 2 (6â€“10 h) â€” Advanced SQL  
  Resources: PostgreSQL [Window Functions](https://www.postgresql.org/docs/current/tutorial-window.html), [EXPLAIN](https://www.postgresql.org/docs/current/using-explain.html).  
  Practice (Deliverable): KPI queries with windows; analyze plans; add indexes and reâ€‘measure.

  
- Week 3 (3â€“8 h) â€” Data Modeling  
  Resource: Kimball [overview](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/).  
  Practice (Deliverable): Design a star schema; ER diagram + rationale.

Supplementary
- [SQLBolt](https://sqlbolt.com/), [Mode SQL](https://mode.com/sql-tutorial/), [LeetCode SQL](https://leetcode.com/studyplan/top-sql-50/)
- Sample DBs: [Chinook](https://github.com/lerocha/chinook-database), [Northwind](https://github.com/microsoft/sql-server-samples/tree/master/samples/databases/northwind-pubs)

Exit Criteria
- Optimize queries, use windows/CTEs, design simple analytical schemas.

---

<a id="stage-6-data-acquisition-web-scraping-and-apis"></a>
### Stage 6 â€” Data Acquisition: Web Scraping and APIs
Estimated effort (total): 18â€“30 h Â· Calendar: ~3â€“6 weeks at 5â€“7 h/week

- Place in our goal: Reliable, ethical ingestion of external data at scale.
- Objectives: Robust scraping, API consumption, ethics/legal, tooling choice.
- Prerequisites: Stage 3

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- MDN: [HTTP overview](https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview), [HTML basics](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics)

Weekly progression
- Week 1 (3â€“6 h) â€” APIs and HTTP Clients  
  Resources: [Requests](https://requests.readthedocs.io/), [httpx](https://www.python-httpx.org/).  
  Practice (Deliverable): Small API client with pagination/auth/retries; readme with usage.

  
- Week 2 (6â€“10 h) â€” Static Scraping  
  Resource: [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).  
  Practice (Deliverable): Extract structured data; persist to CSV/DB; log failures.

  
- Week 3 (6â€“10 h) â€” Dynamic Sites  
  Resources: [Playwright](https://playwright.dev/python/), [Selenium](https://selenium-python.readthedocs.io/).  
  Practice (Deliverable): Headless navigation; capture content behind interactions; robust waits.

  
- Week 4 (3â€“4 h) â€” Crawling and Ethics  
  Resource: [Scrapy](https://docs.scrapy.org/en/latest/); Ethics: [robots.txt](https://www.robotstxt.org/), [guide](https://scrapeops.io/python-scrapy-playbook/python-ethical-web-scraping/).  
  Practice (Deliverable): Spider with throttling/backoff; robots checks; output sample.

Supplementary
- Data Mining â€” [book info](https://www.sciencedirect.com/book/9780123814791/data-mining)

Exit Criteria
- Acquire data responsibly from static/dynamic sources.

---

<a id="stage-7-econometrics-causal-inference-and-time-series"></a>
### Stage 7 â€” Econometrics, Causal Inference, and Time Series
Estimated effort (total): 36â€“64 h Â· Calendar: ~6â€“12 weeks at 5â€“7 h/week

- Place in our goal: Move beyond correlation to credible estimation and sound forecasting. Establishes regression assumptions, identification, and temporal modeling with proper validation.
- Objectives: OLS and diagnostics, common violations and remedies, causal identification basics (DAGs, RCTs, confounding, DiD), timeâ€‘series fundamentals (stationarity, ARIMA), and backtesting.
- Prerequisites: Stage 2 (Statistics), Stage 3 (Programming)

Weekly progression
- Week 1 (8â€“12 h) â€” OLS Foundations and Gaussâ€“Markov  
  Resource: Econometrics with R â€” [Econometrics with R](https://www.econometrics-with-r.org/) (OLS chapters) â€” Why now: Open, applied route to core regression concepts.  
  Practice (Deliverable): Fit OLS; residual diagnostics; interpret coefficients and uncertainty.

  
- Week 2 (6â€“10 h) â€” Diagnostics, Heteroskedasticity, Multicollinearity, Autocorrelation  
  Resources: Econometrics with R (diagnostics), statsmodels examples â€” [statsmodels](https://www.statsmodels.org/stable/index.html)  
  Practice (Deliverable): Breuschâ€“Pagan test; White/HC robust SEs; VIF check; Durbinâ€“Watson; apply appropriate remedy and document rationale.

  
- Week 3 (6â€“10 h) â€” Causal Inference Basics (Identification, DAGs, Omitted Variable Bias)  
  Resource: Cunningham â€” The Mixtape (free) â€” [Causal Inference: The Mixtape](https://mixtape.scunning.com/) â€” Why now: Modern, accessible causal toolkit.  
  Practice (Deliverable): Simulate confounding; show bias under naive OLS; specify DAG; discuss identification strategy.

  
- Week 4 (6â€“10 h) â€” Research Designs: Matching/PS, Differenceâ€‘inâ€‘Differences, Fixed Effects  
  Resource: The Mixtape (DiD/FE chapters); optional: R4DS causal chapters or relevant tutorials.  
  Practice (Deliverable): Implement a 2Ã—2 DiD and a panel FE model on a public dataset; assumption checks; effect interpretation.

  
- Week 5 (6â€“10 h) â€” Time Series Fundamentals (Decomposition, Stationarity, ACF/PACF)  
  Resources: FPP3 (free) â€” [FPP3](https://otexts.com/fpp3/); Python version â€” [Forecasting: The Pythonic Way](https://otexts.com/fpppy/) â€” Why now: Modern forecasting curriculum.  
  Practice (Deliverable): STL decomposition; unitâ€‘root test (ADF); seasonal strength; write diagnostic notes.

  
- Week 6 (4â€“12 h) â€” ARIMA/SARIMA and Backtesting  
  Resources: statsmodels.tsa â€” [statsmodels.tsa](https://www.statsmodels.org/stable/tsa.html)  
  Practice (Deliverable): Fit ARIMA/SARIMA; rollingâ€‘origin backtest; report MAE/MAPE; forecast with intervals; document failure modes.

Supplementary
- Gujarati & Porter â€” Basic Econometrics (reference) â€” [Publisher](https://www.mheducation.com/highered/product/basic-econometrics-gujarati-porter/M9780073375779.html)
- Wooldridge â€” Introductory Econometrics (reference) â€” [Cengage page](https://www.cengage.com/c/introductory-econometrics-a-modern-approach-7e-wooldridge/)
- LÃ¼tkepohl â€” Multiple Time Series (advanced VAR/state space) â€” [Springer](https://link.springer.com/book/10.1007/978-3-540-27752-1)

Exit Criteria
- Diagnose and remedy OLS assumption violations; articulate identification assumptions; implement DiD/FE; build and evaluate ARIMA forecasts with rolling backtests.

---

<a id="stage-8-classical-machine-learning"></a>
### Stage 8 â€” Classical Machine Learning
Estimated effort (total): 30â€“50 h Â· Calendar: ~5â€“9 weeks at 5â€“7 h/week

- Place in our goal: Baseline modeling toolbox and evaluation mindset across domains.
- Objectives: Supervised/unsupervised basics, pipelines, validation, metrics, interpretation.
- Prerequisites: Stage 1â€“2â€“3â€“4

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- Kaggle Intro to ML (4â€“6 h) â€” [course](https://www.kaggle.com/learn/intro-to-machine-learning)  
- StatQuest ML (6â€“12 h skim) â€” [playlist](https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF)

Weekly progression
- Week 1 (12â€“18 h) â€” Supervised Learning  
  Resource: [scikitâ€‘learn](https://scikit-learn.org/stable/) â€” Pipelines, preprocessing, linear/logistic/tree/ensembles.  
  Practice (Deliverable): Full pipeline with ColumnTransformer + CV; baseline and tuned models; model card.

  
- Week 2 (6â€“10 h) â€” Model Selection and Validation  
  Resource: scikitâ€‘learn CV/metrics.  
  Practice (Deliverable): Nested CV vs. holdout; report metric variance and uncertainty.

  
- Week 3 (5â€“8 h) â€” Interpretability  
  Resource: [Interpretable ML](https://christophm.github.io/interpretable-ml-book/).  
  Practice (Deliverable): Permutation importance, PDP/ICE, SHAP; interpretation notes and caveats.

  
- Week 4 (3â€“6 h) â€” Missing Data  
  Resource: [FIMD](https://stefvanbuuren.name/fimd/).  
  Practice (Deliverable): Compare simple imputations vs. MICE; downstream performance and bias discussion.

  
- Week 5 (4â€“8 h) â€” Unsupervised Basics (incl. PCA)  
  Resource: scikitâ€‘learn clustering/dimensionality reduction.  
  Practice (Deliverable): PCA explained variance; customer clustering; silhouette score; UMAP visualization.  
  Optional tieâ€‘in: MML Ch. 10 â€œPrincipal Component Analysisâ€ for deeper LA derivations.

Essential text
- ISLR/ISLRv2 â€” Complete: Ch. 2â€“6 (core); skim Ch. 8 (trees) and Ch. 10 (unsupervised)  
  [Introduction to Statistical Learning](https://www.statlearning.com/)

Supplementary
- [mlcourse.ai](https://mlcourse.ai/book/index.html), [SHAP](https://shap.readthedocs.io/en/latest/)

Exit Criteria
- Ship a reproducible ML pipeline with meaningful evaluation and documented decisions.

---

<a id="stage-9-deep-learning"></a>
### Stage 9 â€” Deep Learning
Estimated effort (total): 40â€“70 h Â· Calendar: ~7â€“11 weeks at 5â€“7 h/week

- Place in our goal: Modern neural architectures and training practices for vision/NLP and beyond.
- Objectives: Neural nets, CNN/RNN basics, modern training, transfer learning, Transformers.
- Prerequisites: Stage 1â€“2â€“3â€“8

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- fast.ai (audit 6â€“10 h skim) â€” [course](https://course.fast.ai/) â€” first 3 lessons  
- Kaggle Intro to DL (3â€“5 h) â€” [course](https://www.kaggle.com/learn/intro-to-deep-learning)

Weekly progression
- Week 1 (12â€“18 h) â€” DL Fundamentals  
  Resources: [D2L](https://d2l.ai/) Ch. 2â€“6; [PyTorch Tutorials](https://pytorch.org/tutorials/) â€œLearn the Basicsâ€.  
  Practice (Deliverable): Implement an MLP; add regularization/schedulers; track metrics in a table.

  
- Week 2 (8â€“12 h) â€” CNN Training  
  Practice (Deliverable): Train a CNN on CIFARâ€‘10; experiment with augmentation and mixup/cutmix; compare runs with logged metrics.

  
- Week 3 (6â€“8 h) â€” Sequence Models  
  Practice (Deliverable): LSTM baseline on a sequence dataset; compare to a classical baseline; error analysis.

  
- Week 4 (6â€“8 h) â€” Transformers Intro  
  Resource: [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).  
  Practice (Deliverable): Fineâ€‘tune a small Transformer on text classification; evaluate; save artifacts.

  
- Week 5 (8â€“14 h) â€” Transfer Learning Project  
  Practice (Deliverable): Endâ€‘toâ€‘end project with dataset prep, training, evaluation, and a demo notebook/app.

Supplementary
- [Deep Learning (Goodfellow et al.)](https://www.deeplearningbook.org/), [CS231n](http://cs231n.stanford.edu/), [fast.ai](https://course.fast.ai/)

Exit Criteria
- Train, debug, and deploy a DL model with solid metrics and documentation.

---

<a id="stage-10-mlops-and-data-engineering"></a>
### Stage 10 â€” MLOps and Data Engineering
Estimated effort (total): 40â€“60 h Â· Calendar: ~6â€“10 weeks at 5â€“7 h/week

- Place in our goal: Take projects to production with reproducibility, automation, and scalable data pipelines.
- Objectives: Experiment tracking, model/data versioning, containers, CI/CD, orchestration; batch/stream pipelines, warehouses, transformations, Spark.
- Prerequisites: Stage 8â€“9

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- Docker 101 (2â€“4 h) â€” [tutorial](https://www.docker.com/101-tutorial/), GH Actions Quickstart (1â€“2 h) â€” [guide](https://docs.github.com/en/actions/quickstart)

Weekly progression
- Week 1 (6â€“8 h) â€” Experiment Tracking  
  Resource: [MLflow](https://mlflow.org/) â€” Tracking + Models + Registry.  
  Practice (Deliverable): Track runs/artifacts; compare experiments; promote best model to registry.

  
- Week 2 (6â€“8 h) â€” Data/Model Versioning  
  Resource: [DVC](https://dvc.org/) â€” Get Started + Pipelines + Remote.  
  Practice (Deliverable): Version datasets; create pipelines; reproduce results endâ€‘toâ€‘end.

  
- Week 3 (6â€“8 h) â€” Containerization  
  Resource: [Docker â€“ Get Started](https://docs.docker.com/get-started/) â€” best practices.  
  Practice (Deliverable): Containerize your ML project; validate locally and in CI.

  
- Week 4 (5â€“8 h) â€” CI/CD and Orchestration  
  Resources: GH Actions; [Airflow](https://airflow.apache.org/) / [Prefect](https://docs.prefect.io/); [Great Expectations](https://docs.greatexpectations.io/).  
  Practice (Deliverable): Weekly batch job with data checks and model refresh; passing CI.

  
- Week 5 (8â€“12 h) â€” Warehousing and Transformations  
  Resource: [dbt Fundamentals](https://docs.getdbt.com/docs/get-started-dbt).  
  Practice (Deliverable): Staging/model layer with tests/docs in dbt; exposures.

  
- Week 6 (8â€“12 h) â€” Spark and Streaming  
  Resource: Data Engineering Zoomcamp (Spark, Kafka) â€” [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)  
  Practice (Deliverable): Spark ETL job; benchmark vs. pandas; simple Kafka ingestion; checkpointing.

Supplementary
- [Machine Learning Systems](https://mlsysbook.ai/)
- [Delta Lake](https://delta.io/), [Apache Iceberg](https://iceberg.apache.org/) â€” Lakehouse patterns.

Exit Criteria
- From notebook to reproducible, testable, containerized service with automated data/ML pipelines.

---

## Specialization Tracks (optional)

> Optional after Stage 10 or in parallel where relevant.

<a id="b-nlp-and-llms"></a>
### B) NLP and LLMs
Estimated effort (total): 24â€“40 h

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- spaCy Course (3â€“6 h) â€” [course](https://course.spacy.io/en/)

Weekly progression
- Week B1 (10â€“16 h) â€” Transformers Fundamentals  
  Resource: [Hugging Face Course](https://huggingface.co/learn/nlp-course/chapter1) â€” Ch. 1â€“4.  
  Practice (Deliverable): Fineâ€‘tune a text classifier; track metrics; export artifacts.

  
- Week B2 (6â€“10 h) â€” RAG Systems  
  Resources: [LangChain](https://python.langchain.com/), [LlamaIndex](https://docs.llamaindex.ai/).  
  Practice (Deliverable): RAG app on your docs; evaluation with ragas; latency/quality tradeâ€‘offs.

  
- Week B3 (6â€“10 h) â€” Evaluation and Safety  
  Resources: [lmâ€‘evalâ€‘harness](https://github.com/EleutherAI/lm-eval-harness), [ragas](https://github.com/explodinggradients/ragas), [NIST AI RMF](https://www.nist.gov/itl/ai-risk-management-framework).  
  Practice (Deliverable): Build an evaluation suite; document safety mitigations.

  
- Week B4 (2â€“4 h, optional) â€” Agents  
  Resource: [HF Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction).  
  Practice (Deliverable): Prototype a simple agent with a constrained toolset.

Exit Criteria
- Endâ€‘toâ€‘end RAG with evaluation and basic safety.

---

<a id="c-computer-vision"></a>
### C) Computer Vision
Estimated effort (total): 24â€“40 h

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- PyTorch 60â€‘min Blitz (2â€“4 h) â€” [tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)

Weekly progression
- Week C1 (10â€“16 h) â€” Vision Foundations  
  Resource: [torchvision tutorials](https://pytorch.org/vision/stable/index.html#tutorials).  
  Practice (Deliverable): Train a ResNet with augmentations; evaluate with confusion matrix/AUC.

  
- Week C2 (6â€“10 h) â€” Practical DL  
  Resources: [fastai vision](https://docs.fast.ai/vision.learner.html), [course](https://course.fast.ai/).  
  Practice (Deliverable): Prototype multiple architectures; compare results.

  
- Week C3 (4â€“8 h, optional) â€” Theory  
  Resource: [CS231n](http://cs231n.stanford.edu/).  
  Practice (Deliverable): Custom augmentation/evaluation protocol and brief report.

Exit Criteria
- Fineâ€‘tuned vision model with clear evaluation.

---

<a id="d-recommender-systems"></a>
### D) Recommender Systems
Estimated effort (total): 18â€“30 h

Zeroâ€‘toâ€‘One Onâ€‘Ramp
- Recsys basics (2â€“4 h) â€” [Google Developers](https://developers.google.com/machine-learning/recommendation/collaborative/basics)

Weekly progression
- Week D1 (6â€“10 h) â€” MF and Implicit Feedback  
  Resource: [implicit](https://github.com/benfred/implicit).  
  Practice (Deliverable): Train ALS/BPR on interactions; tune hyperparams; offline metrics.

  
- Week D2 (6â€“10 h) â€” Pipelines and Evaluation  
  Resource: [Microsoft Recommenders](https://github.com/microsoft/recommenders).  
  Practice (Deliverable): Offline eval pipeline; MAP/NDCG/Recall@k; ablations.

  
- Week D3 (6â€“10 h) â€” Ranking Metrics  
  Resource: [Metrics overview (PDF)](https://cmci.colorado.edu/classes/INFO-4604/files/rec_sys_metrics.pdf).  
  Practice (Deliverable): Compare candidate generators/rankers with proper ranking metrics.

Exit Criteria
- Topâ€‘N recommender with offline eval and simple online serving.

---

## Capstone and Portfolio

Deliverables
- 1 Capstone (endâ€‘toâ€‘end): problem framing â†’ data â†’ modeling â†’ deployment â†’ docs
- 2â€“3 polished midâ€‘size projects from earlier stages

Checklist
- Clear README, architecture diagram, environment file, tests, Makefile/CLI
- Reproducible runs, tracked experiments, meaningful metrics, demo (app/notebook)

Presentation
- Oneâ€‘page case study blog per project, emphasizing decisions, uncertainty, and impact.

---

## Appendix

- Datasets
  - [UCI Machine Learning Repository](https://archive.ics.uci.edu/) â€” Curated datasets for benchmarking.
  - [Kaggle Datasets](https://www.kaggle.com/datasets) â€” Variety + public notebooks.
  - [Google Dataset Search](https://datasetsearch.research.google.com/) â€” Metaâ€‘search to find domain data.
- Templates
  - [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/) â€” Standardized project structure.
- Study tips
  - Timeboxing, spaced repetition, â€œprojectâ€‘firstâ€ learning â€” Improves retention and portfolio output.

# AI & Data Science Weekly Plan â€” Activities, Practice, and Pass Criteria

![Duration](https://img.shields.io/badge/duration-~164_weeks-6f42c1)
![Pace](https://img.shields.io/badge/pace-8â€“10_h%2Fweek-0e8a16)
![Path](https://img.shields.io/badge/path-beginner%E2%86%92practitioner-0366d6)
![Style](https://img.shields.io/badge/style-cumulative%2C_concept%E2%86%92practice-555)

Zero prior knowledge is assumed. Learning order is strictly top-to-bottom. Each week includes a clear â€œPassâ€ requirement aligned to the primary resource.

â€” Quick jump â€”
- Phase 1 Â· Data Analysis Foundations
- Phase 2 Â· Mathematics for ML
- Phase 3 Â· Statistics Fundamentals
- Phase 4 Â· Bayesian Statistics & Missing Data
- Phase 5 Â· Statistical Learning with Python (ISLP)
- Phase 6 Â· Classical ML
- Phase 7 Â· Data Mining
- Phase 8 Â· Econometrics & Time Series
- Phase 9 Â· R for Data Science
- Phase 10 Â· Web Scraping & SQL
- Phase 11 Â· Deep Learning
- Phase 12 Â· MLOps & Data Engineering
- Phase 13 Â· LLMs & Open-Source AI
- Phase 14 Â· Consolidation & Capstone

Legend
- ğŸ“– Activities (primary source)
- ğŸ§ª Practice (small tasks)
- âœ… Pass (weekly pass criterion)
- ğŸ› ï¸ How (implementation hint)
- ğŸ” Flex (catch-up, spaced review)

Duration and pacing
- Duration: ~164 weeks (â‰ˆ3.1 years), 8â€“10 h/week
- Weekly output: small practical tasks only
- Frequent Flex Weeks between phases for consolidation

Main resources (cover-to-cover completion)
- Python for Data Analysis â€” Wes McKinney â€” [Python for Data Analysis](https://wesmckinney.com/book/)
- Mathematics for Machine Learning â€” Deisenroth, Faisal, Ong â€” [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- Think Stats â€” Allen B. Downey â€” [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- Think Bayes â€” Allen B. Downey â€” [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- Flexible Imputation of Missing Data â€” van Buuren â€” [FIMD](https://stefvanbuuren.name/fimd/)
- An Introduction to Statistical Learning with Applications in Python â€” James, Witten, Hastie, Tibshirani â€” [ISLP](https://www.statlearning.com/)
- Pattern Recognition and Machine Learning â€” Bishop â€” [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- Interpretable Machine Learning â€” Molnar â€” [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- Data Mining: Concepts and Techniques (3e) â€” Han, Kamber, Pei â€” [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- Basic Econometrics â€” Gujarati â€” [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- New Introduction to Multiple Time Series â€” LÃ¼tkepohl â€” [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- R for Data Science (2e) â€” Wickham, Ã‡etinkaya-Rundel, Grolemund â€” [R for Data Science (2e)](https://r4ds.hadley.nz)
- Beautiful Soup docs â€” [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- Selenium (Python) docs â€” [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- SQL Tutorial â€” [SQL Tutorial](https://www.sqltutorial.org/)
- Dive into Deep Learning â€” Zhang et al. â€” [D2L](https://d2l.ai)
- Deep Learning â€” Goodfellow, Bengio, Courville â€” [Deep Learning Book](https://www.deeplearningbook.org/)
- MLOps Zoomcamp â€” DataTalksClub â€” [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- Machine Learning Systems â€” Symeonidis et al. â€” [ML Systems](https://mlsysbook.ai)
- Data Engineering Zoomcamp â€” DataTalksClub â€” [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- Hugging Face Course â€” [HF Course](https://huggingface.co/course/chapter1)
- HF Agents Course â€” [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)

Supporting references (selective)
- Trigonometric Cheat Sheet â€” [Trig Sheet (PDF)](https://tutorial.math.lamar.edu/pdf/Trig_Cheat_Sheet.pdf)
- Python Crash Course â€” [Video](https://www.youtube.com/watch?v=rfscVS0vtbw)
- Kevin Sheppard Python Notes â€” [Notes (PDF)](https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2021.pdf)
- PSU STAT â€” [STAT portal](https://online.stat.psu.edu)
- scikit-learn docs â€” [scikit-learn](https://scikit-learn.org/stable/index.html)
- statsmodels docs â€” [statsmodels](https://www.statsmodels.org/stable/index.html)

---------------------------------------------------------------------

<details>
<summary><b>Phase 1 Â· Data Analysis Foundations â€” Weeks 1â€“8 (Complete Python for Data Analysis)</b></summary>

Week 1 â€” P4DA Ch. 1â€“2
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Set up Python environment (conda/pip); run IPython/Jupyter; practice Python basics (variables, control flow, functions); understand the data analysis ecosystem.
- âœ… Pass: Create a notebook demonstrating Python fundamentals: define 3 functions, use list/dict comprehensions, write a simple script that reads command-line arguments, and explain the role of NumPy/pandas/matplotlib in the data stack.
- ğŸ› ï¸ How: Install Anaconda or miniconda; launch Jupyter; experiment with built-in types and control structures; skim the library overview in Ch.1.

Week 2 â€” P4DA Ch. 3â€“4
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Work with tuples, lists, dicts, sets (Ch.3); create and manipulate NumPy ndarrays; practice array indexing, slicing, and vectorized operations (Ch.4).
- âœ… Pass: Build a notebook that: (1) demonstrates list/dict/set operations; (2) creates 2D NumPy arrays, performs element-wise and matrix operations; (3) uses boolean indexing to filter data; (4) times vectorized vs loop-based computation.
- ğŸ› ï¸ How: `np.array`, `np.arange`, `np.reshape`, boolean masks, `np.where`, `%timeit` to compare performance.

Week 3 â€” P4DA Ch. 5â€“6
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Create Series and DataFrames; use `.loc/.iloc` indexing; load data from CSV/JSON/Excel files (Ch.5â€“6).
- âœ… Pass: Load a dataset from CSV, inspect with `.head()/.info()/.describe()`, select columns via `.loc/.iloc`, filter rows with boolean masks, and export cleaned data to a new CSV.
- ğŸ› ï¸ How: `pd.read_csv`, `pd.read_json`, `df.loc[rows, cols]`, `df.iloc[row_idx, col_idx]`, `df.to_csv`.

Week 4 â€” P4DA Ch. 7â€“8
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Handle missing data; clean strings with `.str` methods; merge/join DataFrames; reshape with `stack/unstack/pivot/melt` (Ch.7â€“8).
- âœ… Pass: Take a messy dataset and: (1) handle missing values (drop or fill); (2) standardize string columns (trim/lower); (3) merge with a second table; (4) pivot or melt the result; document row counts at each step.
- ğŸ› ï¸ How: `df.dropna`, `df.fillna`, `df["col"].str.strip().str.lower()`, `pd.merge`, `pd.pivot_table`, `pd.melt`.

Week 5 â€” P4DA Ch. 9â€“10
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Create plots with matplotlib/seaborn (Ch.9); perform aggregation with `groupby` (Ch.10).
- âœ… Pass: Produce 4 visualizations (histogram, scatter, line, bar) with proper labels/titles; use `groupby().agg()` to compute multi-column summaries; combine groupby results with plots.
- ğŸ› ï¸ How: `plt.plot`, `plt.hist`, `sns.scatterplot`, `df.groupby("col").agg({"num":"mean"})`, `plt.savefig`.

Week 6 â€” P4DA Ch. 11â€“12 (+appendices)
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Work with time series: DateTimeIndex, resampling, rolling windows (Ch.11); explore advanced pandas: Categoricals, method chaining, performance (Ch.12).
- âœ… Pass: Load time series data, set DateTimeIndex, resample to weekly/monthly, compute rolling statistics; convert a column to Categorical; refactor pipeline using method chaining; time vectorized vs apply.
- ğŸ› ï¸ How: `pd.to_datetime`, `df.set_index`, `df.resample("W").mean()`, `.rolling(7).mean()`, `pd.Categorical`, `.pipe()`.

Week 7 â€” P4DA Project A
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: End-to-end EDA pipeline using all chapters 1â€“12: load, clean, transform, aggregate, visualize.
- âœ… Pass: Apply a complete EDA workflow to a new dataset; produce â‰¥5 visualizations; write a 1-page summary with â‰¥3 insights, â‰¥2 hypotheses, and â‰¥1 data quality issue identified.
- ğŸ› ï¸ How: Combine prior weeks' functions into a reusable pipeline; keep code modular and well-documented.

Week 8 â€” P4DA Project B
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Feature engineering using transforms from the book: date/time features, categorical encoding, ratios, binning.
- âœ… Pass: Create â‰¥5 derived features (date parts, ratios, binned numerics, category combinations); document each feature's rationale, potential predictive value, and leakage risk.
- ğŸ› ï¸ How: `df["date"].dt.month`, `df.assign(ratio=lambda x: x["a"]/x["b"])`, `pd.cut`, `pd.get_dummies`.
</details>

ğŸ” Flex â€” Consolidate EDA template and notes

---------------------------------------------------------------------

<details>
<summary><b>Phase 2 Â· Mathematics for ML â€” Weeks 9â€“18 (Complete MML)</b></summary>

Week 9 â€” Linear Algebra I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Vectors (addition, scalar multiplication, norms); matrix operations (transpose, multiplication); linear independence and basis.
- âœ… Pass: Implement vector/matrix operations from scratch; verify linear independence of a set of vectors; compute and interpret different vector norms (L1, L2, Linf).
- ğŸ› ï¸ How: `np.dot`, `np.linalg.norm`, `np.linalg.matrix_rank`; manually verify independence via row reduction.

Week 10 â€” Linear Algebra II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Eigenvalues and eigenvectors; matrix diagonalization; positive definiteness; condition number.
- âœ… Pass: Compute eigendecomposition of symmetric matrices; verify diagonalization A = PDPâ»Â¹; check positive definiteness via eigenvalues; interpret condition number for numerical stability.
- ğŸ› ï¸ How: `np.linalg.eig`, `np.linalg.eigh` for symmetric; `np.linalg.cond`; verify reconstruction.

Week 11 â€” Decompositions & Geometry
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: SVD and its applications; orthogonal projections; analytic geometry (distances, angles, hyperplanes).
- âœ… Pass: Compute SVD; reconstruct matrix from top-k components and plot reconstruction error vs k; project points onto a subspace; compute distances to hyperplanes.
- ğŸ› ï¸ How: `np.linalg.svd`; projection formula; `np.linalg.lstsq` for least squares via normal equations and QR.

Week 12 â€” Vector Calculus I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Partial derivatives; gradients of scalar functions; Jacobians of vector functions.
- âœ… Pass: Compute gradients analytically for multivariate functions; verify with numerical finite differences; visualize gradient field on a contour plot.
- ğŸ› ï¸ How: Derive gradient by hand; implement central differences `(f(x+h)-f(x-h))/(2h)`; `plt.contour` with `plt.quiver`.

Week 13 â€” Vector Calculus II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Chain rule for composed functions; backpropagation intuition; Hessians and second-order derivatives.
- âœ… Pass: Derive gradients of composed functions using chain rule; compute Hessian matrix; verify gradient computation with central-difference check (max abs diff < 1e-4).
- ğŸ› ï¸ How: Symbolic differentiation by hand; numerical Hessian via finite differences; check gradient correctness.

Week 14 â€” Probability I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Random variables; probability distributions (discrete and continuous); expectation and variance; common distributions (Bernoulli, Binomial, Gaussian).
- âœ… Pass: Simulate samples from common distributions; compute empirical vs theoretical mean/variance; verify Law of Large Numbers by plotting sample mean convergence.
- ğŸ› ï¸ How: `np.random`, `scipy.stats`; compare empirical moments to closed-form expressions.

Week 15 â€” Probability II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Joint and marginal distributions; covariance and correlation; multivariate Gaussian; Gaussian conditioning and marginalization.
- âœ… Pass: Generate correlated Normals via Cholesky decomposition; recover empirical covariance matrix; visualize 2D Gaussian contours; demonstrate conditioning a multivariate Gaussian.
- ğŸ› ï¸ How: `L = np.linalg.cholesky(Sigma)`; `X = Z @ L.T`; `np.cov`; contour plots for bivariate Gaussian.

Week 16 â€” Optimization I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Convex functions and sets; gradient descent algorithm; learning rate selection; convergence analysis.
- âœ… Pass: Implement gradient descent for a convex quadratic f(x)=Â½x^TQx+c^Tx; show monotone loss decrease; experiment with different step sizes and plot convergence curves.
- ğŸ› ï¸ How: Analytic gradient Qx+c; fixed and adaptive step sizes; plot loss vs iterations.

Week 17 â€” Optimization II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Newton's method; constrained optimization concepts; regularization and its geometric interpretation.
- âœ… Pass: Implement Newton's method using Hessian; compare convergence (iterations to tolerance) with gradient descent; solve ridge regression and visualize how Î» affects the solution.
- ğŸ› ï¸ How: Newton step: x_new = x - Hâ»Â¹âˆ‡f; `scipy.optimize.minimize`; compare first-order vs second-order methods.

Week 18 â€” Review
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Create concept map linking all MML topics; write summary notes connecting math foundations to ML applications.
- âœ… Pass: A one-page concept map with â‰¥10 explicit connections between math concepts and ML techniques (e.g., eigenvalues â†” PCA, gradient descent â†” neural network training, condition number â†” numerical stability).
- ğŸ› ï¸ How: Use mind-mapping tool or hand-drawn diagram; include concrete examples for each link.
</details>

ğŸ” Flex â€” Retrieval practice and summaries

---------------------------------------------------------------------

<details>
<summary><b>Phase 3 Â· Statistics Fundamentals â€” Weeks 19â€“24 (Complete Think Stats)</b></summary>

Week 19 â€” Think Stats Ch. 1
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Explore a dataset; compute summary statistics; build histograms and PMFs; construct ECDFs.
- âœ… Pass: Implement ECDF from scratch on real data; verify it is non-decreasing and ends at 1.0; overlay histogram and ECDF to compare distributional insights; interpret outliers.
- ğŸ› ï¸ How: `np.sort`; `np.arange(1,n+1)/n`; `plt.step` for ECDF; `plt.hist` for histogram.

Week 20 â€” Think Stats Ch. 2
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Compute central tendency (mean, median, mode) and spread (variance, std, range, IQR); explore effect of outliers on these measures.
- âœ… Pass: Compare mean/SD vs median/MAD/IQR on 2 datasets (one symmetric, one skewed); explain when each measure is appropriate; show outlier impact graphically.
- ğŸ› ï¸ How: `np.mean`, `np.median`, `np.std`; `scipy.stats.median_abs_deviation`; `np.percentile` for IQR.

Week 21 â€” Think Stats Ch. 3â€“4
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Work with CDFs and PMFs; model data with probability distributions; compare empirical vs theoretical distributions.
- âœ… Pass: Fit data to common distributions (Normal, Exponential); use CDF plots to assess fit; compute percentiles and quantiles; explain when to use PMF vs CDF.
- ğŸ› ï¸ How: `scipy.stats.norm.fit`, `scipy.stats.expon`; `probplot` for QQ plots; CDF comparison plots.

Week 22 â€” Think Stats Ch. 5â€“6
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Model data with analytical distributions; explore relationships between variables; compute conditional probabilities.
- âœ… Pass: Fit a parametric model to real data; compute and interpret correlation and covariance; demonstrate conditional probability with a contingency table.
- ğŸ› ï¸ How: `scipy.stats` distribution fitting; `np.corrcoef`; `pd.crosstab` for contingency tables.

Week 23 â€” Think Stats Ch. 7â€“8
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Estimate parameters with confidence intervals; perform hypothesis tests; understand p-values and statistical significance.
- âœ… Pass: Compute confidence intervals via bootstrap and analytical methods; run a hypothesis test; simulate to show Type I error â‰ˆ Î±; produce a power curve for detecting effect sizes.
- ğŸ› ï¸ How: Bootstrap resampling; `scipy.stats.ttest_ind`; simulation to count rejections under Hâ‚€ and Hâ‚.

Week 24 â€” Think Stats Ch. 9â€“10 (+wrap)
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: Explore linear relationships; fit simple and multiple regression; interpret coefficients; check regression assumptions.
- âœ… Pass: Fit OLS regression; interpret RÂ², coefficients, and p-values; produce diagnostic plots (residuals vs fitted, QQ plot); compute VIFs and flag multicollinearity.
- ğŸ› ï¸ How: `statsmodels.api.OLS`; `statsmodels.stats.outliers_influence.variance_inflation_factor`; diagnostic plots.
</details>

ğŸ” Flex â€” Stats recap

---------------------------------------------------------------------

<details>
<summary><b>Phase 4 Â· Bayesian & Missing Data â€” Weeks 25â€“36 (Complete Think Bayes, FIMD)</b></summary>

Weeks 25â€“32 â€” Think Bayes (Ch. 1â€“14, paced)
- ğŸ“– [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- ğŸ§ª Practice: Apply Bayes' theorem to update beliefs; implement conjugate prior models (Beta-Binomial, Gamma-Poisson, Normal-Normal); perform posterior predictive checks; compare models.
- âœ… Pass (weekly): Implement a Bayesian model aligned with the chapter's topic; show prior sensitivity analysis (vary prior parameters and observe posterior changes); generate posterior predictive samples and compare to observed data using a suitable test statistic.
- ğŸ› ï¸ How: Use analytical posteriors when available; for PPC, draw samples from posterior, then from likelihood, and compare summary stats to data.

Weeks 33â€“36 â€” Flexible Imputation of Missing Data (complete)
- ğŸ“– [FIMD](https://stefvanbuuren.name/fimd/)
- ğŸ§ª Practice: Missingness mechanisms; MICE; sensitivity (as in book)
- âœ… Pass (weekly): Run MICE (mâ‰¥5) on a dataset; report pooled estimates per Rubinâ€™s rules; compare to complete-case; perform delta-adjustment sensitivity where relevant.
- ğŸ› ï¸ How: use a MICE implementation (e.g., statsmodels/impyute/sklearn-iterative as proxy) consistent with book procedures.
</details>

ğŸ” Flex â€” Consolidate Bayesian + MI

---------------------------------------------------------------------

<details>
<summary><b>Phase 5 Â· Statistical Learning with Python â€” Weeks 37â€“46 (Complete ISLP)</b></summary>

Week 37 â€” ISLP Ch. 1â€“2 (Intro + Statistical Learning)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Understand the statistical learning framework; implement train/test splits; explore the bias-variance trade-off with KNN at different k values.
- âœ… Pass: On a dataset, demonstrate how training error decreases with model complexity while test error shows U-shape; implement 5-fold CV and compare to hold-out estimate; discuss flexibility vs interpretability.
- ğŸ› ï¸ How: `train_test_split`; `KFold`/`cross_val_score`; vary KNN's k parameter; plot training vs test error curves.

Week 38 â€” ISLP Ch. 3 (Linear Regression)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Fit simple and multiple linear regression; interpret coefficients; add interaction and polynomial terms; assess model fit with residual diagnostics.
- âœ… Pass: Fit OLS with and without interaction/polynomial terms; compare RÂ² vs adjusted RÂ²; produce residual plots; select optimal polynomial degree via CV; interpret coefficient confidence intervals.
- ğŸ› ï¸ How: `LinearRegression`; `PolynomialFeatures`; `cross_val_score`; `statsmodels` for CIs; residual diagnostics.

Week 39 â€” ISLP Ch. 4 (Classification)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Implement logistic regression; understand LDA/QDA assumptions; apply KNN for classification; explore classification metrics beyond accuracy.
- âœ… Pass: Compare logistic regression, LDA, QDA, and KNN using stratified 5-fold CV; report confusion matrix, precision, recall, and ROC-AUC; select optimal classification threshold based on problem context.
- ğŸ› ï¸ How: `LogisticRegression`; `LinearDiscriminantAnalysis`; `QuadraticDiscriminantAnalysis`; `KNeighborsClassifier`; `roc_curve` for threshold selection.

Week 40 â€” ISLP Ch. 5 (Resampling Methods)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Compare validation approaches: hold-out, LOOCV, k-fold CV; use bootstrap for uncertainty estimation; understand variance-bias trade-off in resampling.
- âœ… Pass: Compare test error estimates from LOOCV vs 5-fold vs 10-fold CV; implement bootstrap to estimate coefficient standard errors; compare bootstrap SEs to analytic SEs.
- ğŸ› ï¸ How: `LeaveOneOut`; `KFold`; implement bootstrap loop with `np.random.choice`; fix seeds for reproducibility.

Week 41 â€” ISLP Ch. 6 (Model Selection & Regularization)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Understand the motivation for regularization; implement ridge and lasso regression; interpret coefficient shrinkage and sparsity; tune regularization parameter via CV.
- âœ… Pass: Plot ridge and lasso coefficient paths as Î» varies; select optimal Î» via CV; compare test error of OLS vs ridge vs lasso; explain when lasso produces sparse solutions.
- ğŸ› ï¸ How: `Ridge`; `Lasso`; `RidgeCV`; `LassoCV`; `StandardScaler` (scale features first); `lasso_path` for path plots.

Week 42 â€” ISLP Ch. 7 (Beyond Linearity)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Move beyond linearity with polynomial regression, step functions, and splines; understand degrees of freedom; fit GAM-style models.
- âœ… Pass: Fit polynomial, step function, and spline models; compare their flexibility and test errors; produce partial dependence plots; select appropriate number of knots/degrees via CV.
- ğŸ› ï¸ How: `PolynomialFeatures`; `SplineTransformer`; `pd.cut` for step functions; compare MSE on held-out data.

Week 43 â€” ISLP Ch. 8 (Tree-Based Methods)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Fit decision trees; understand bagging and the random forest algorithm; implement gradient boosting; interpret tree-based models.
- âœ… Pass: Fit and prune a decision tree; compare single tree vs random forest vs gradient boosting on test error; show OOB error for RF; plot feature importances and partial dependence plots.
- ğŸ› ï¸ How: `DecisionTreeClassifier/Regressor`; `RandomForestClassifier/Regressor`; `GradientBoostingClassifier/Regressor`; `permutation_importance`; `plot_partial_dependence`.

Week 44 â€” ISLP Ch. 9 (Support Vector Machines)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Understand maximal margin classifiers and support vectors; fit SVMs with linear and non-linear kernels; tune hyperparameters (C, gamma).
- âœ… Pass: Fit SVM with linear and RBF kernels; tune C and gamma via grid search with CV; visualize decision boundaries on 2D data; identify and highlight support vectors; compare to logistic regression.
- ğŸ› ï¸ How: `SVC`; `GridSearchCV`; `plt.contourf` for decision boundaries; access `support_vectors_` attribute.

Week 45 â€” ISLP Ch. 10 (Unsupervised Learning)
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Perform dimensionality reduction with PCA; apply k-means and hierarchical clustering; understand the importance of scaling; evaluate clustering quality.
- âœ… Pass: Apply PCA and plot cumulative explained variance; choose number of components; cluster with k-means (elbow method for k) and hierarchical clustering (dendrogram); evaluate with silhouette score and compare cluster stability across random seeds.
- ğŸ› ï¸ How: `StandardScaler` (always scale first); `PCA`; `KMeans` with inertia plots; `AgglomerativeClustering`; `dendrogram`; `silhouette_score`.

Week 46 â€” ISLP Labs/Wrap-up
- ğŸ“– Activities: [ISLP](https://www.statlearning.com/)
- ğŸ§ª Practice: Complete an end-to-end ML project using techniques from all ISLP chapters: EDA, preprocessing, model selection, hyperparameter tuning, evaluation, and interpretation.
- âœ… Pass: Deliver a reproducible notebook with proper train/test split, cross-validation, model comparison, hyperparameter tuning, error analysis, and a 1-page summary documenting decisions, limitations, and risks.
- ğŸ› ï¸ How: `Pipeline`; `ColumnTransformer` for mixed feature types; `GridSearchCV`/`RandomizedSearchCV`; fixed `random_state` throughout; clean documentation.
</details>

ğŸ” Flex â€” Validation basics consolidation

---------------------------------------------------------------------

<details>
<summary><b>Phase 6 Â· Classical ML â€” Weeks 47â€“65 (Complete PRML, Interpretable ML)</b></summary>

Weeks 47â€“60 â€” PRML (Ch. 1â€“13 + review)
- ğŸ“– [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- ğŸ§ª Practice: Implement core algorithms from each chapter from scratch: probability distributions, linear models, neural networks, kernel methods, graphical models, mixture models, EM algorithm, approximate inference, and sampling methods.
- âœ… Pass (weekly): Implement the chapter's focal algorithm from scratch; verify correctness by comparing to sklearn/scipy baseline (within 2-5% accuracy); document mathematical derivations; use fixed seeds for reproducibility.
- ğŸ› ï¸ How: Use NumPy for implementations; sklearn only as verification oracle; work on toy datasets; keep detailed notes linking code to book equations.

Weeks 61â€“65 â€” Interpretable ML (complete)
- ğŸ“– [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- ğŸ§ª Practice: Apply model-agnostic interpretation methods: PDP, ICE, permutation importance, LIME, SHAP; understand intrinsically interpretable models; explore feature interaction methods.
- âœ… Pass (weekly): For a trained model, produce PDP/ICE plots for top features; compute permutation importance; generate SHAP values for individual predictions; write a 1-page analysis comparing methods' stability across 3 bootstrap resamples.
- ğŸ› ï¸ How: `sklearn.inspection.PartialDependenceDisplay`; `permutation_importance`; `shap.Explainer`; compare explanations across train/test sets.
</details>

ğŸ” Flex â€” Validation & interpretation synthesis

---------------------------------------------------------------------

<details>
<summary><b>Phase 7 Â· Data Mining â€” Weeks 66â€“74 (Complete DM 3e)</b></summary>

Weeks 66â€“74 â€” Data Mining 3e (Ch. 1â€“12)
- ğŸ“– [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- ğŸ§ª Practice: Per-chapter algorithmic work strictly matching the chapter (e.g., data preprocessing tasks; Apriori/FP-Growth; decision trees; k-means/DBSCAN; outlier detection)
- âœ… Pass (weekly): Implement a minimal working version for the chapterâ€™s focal algorithm OR replicate results using a library; verify correctness on a deterministic toy and compare performance on a small real dataset.
- ğŸ› ï¸ How: construct small synthetic datasets with known ground truth (fixed seeds); assert counts/clusters/rules match expectation.
</details>

ğŸ” Flex â€” Mining recap

---------------------------------------------------------------------

<details>
<summary><b>Phase 8 Â· Econometrics & Time Series â€” Weeks 75â€“96 (Complete Gujarati, LÃ¼tkepohl)</b></summary>

Weeks 75â€“86 â€” Basic Econometrics (complete)
- ğŸ“– [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- ğŸ§ª Practice: Reproduce a worked example per chapter using methods from that chapter only (OLS basics; classical assumption diagnostics; heteroskedasticity/autocorrelation remedies; functional form; limited dependent variables as presented)
- âœ… Pass (weekly): Match the textbook exampleâ€™s coefficients and standard errors (within rounding) and include one robustness check discussed in that chapter (e.g., robust/HAC SEs when appropriate).
- ğŸ› ï¸ How: `statsmodels` OLS/GLM, `cov_type="HC3"` or HAC if the chapter addresses it; include diagnostic plots taught there.

Weeks 87â€“96 â€” LÃ¼tkepohl (complete)
- ğŸ“– [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- ğŸ§ª Practice: Implement multivariate time series analysis: VAR model specification, estimation, lag order selection, stability analysis, impulse response functions, forecast error variance decomposition, and cointegration/VECM.
- âœ… Pass (weekly): Fit VAR/VECM to macroeconomic data; select lag order using information criteria; verify stability (roots inside unit circle); compute and plot IRFs with confidence bands; perform Johansen cointegration test when applicable.
- ğŸ› ï¸ How: `statsmodels.tsa.api.VAR`; `statsmodels.tsa.vector_ar.vecm.VECM`; `irf()` for impulse responses; rolling-window forecasts for evaluation.
</details>

ğŸ” Flex â€” Econometrics/time-series consolidation

---------------------------------------------------------------------

<details>
<summary><b>Phase 9 Â· R for Data Science â€” Weeks 97â€“106 (Complete R4DS 2e)</b></summary>

Weeks 97â€“106 â€” R4DS (Complete)
- ğŸ“– [R for Data Science (2e)](https://r4ds.hadley.nz)
- ğŸ§ª Practice: Learn R and tidyverse progressively: data import, tidying (pivot_longer/wider), transformation (dplyr verbs), visualization (ggplot2), strings, factors, dates, functions, iteration, and communication (Quarto/RMarkdown).
- âœ… Pass (weekly): Complete a mini-analysis using only functions from chapters covered that week; produce a Quarto/RMarkdown report that renders end-to-end; include at least one visualization and one summary table.
- ğŸ› ï¸ How: `library(tidyverse)`; `read_csv`; `dplyr` verbs (`filter`, `mutate`, `summarize`, `group_by`); `ggplot2`; `set.seed()` for reproducibility.
</details>

ğŸ” Flex â€” R consolidation

---------------------------------------------------------------------

<details>
<summary><b>Phase 10 Â· Web Scraping & SQL â€” Weeks 107â€“112 (Complete BeautifulSoup, Selenium, SQL)</b></summary>

Week 107 â€” BeautifulSoup
- ğŸ“– [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- ğŸ§ª Practice: Scrape static HTML pages: fetch with requests, parse with BeautifulSoup, navigate the DOM, extract data using CSS selectors and tag methods.
- âœ… Pass: Scrape a static website and extract structured data; save as CSV/JSON with documented schema; check robots.txt before scraping; implement polite delays to avoid rate limiting (no HTTP 429 errors).
- ğŸ› ï¸ How: `requests.get(url)`; `BeautifulSoup(html, "lxml")`; `.select()` for CSS selectors; `.find_all()` for tag-based search; `time.sleep()` between requests.

Weeks 108â€“109 â€” Selenium
- ğŸ“– [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- ğŸ§ª Practice: Automate browser interactions for dynamic websites: handle JavaScript-rendered content, implement explicit waits, manage pagination and infinite scroll, fill forms.
- âœ… Pass (weekly): Scrape a JavaScript-rendered page (e.g., infinite scroll or content behind clicks); implement proper waits and error handling; save timestamped data with retry/timeout logs; handle at least one failure scenario gracefully.
- ğŸ› ï¸ How: `webdriver.Chrome()`; `WebDriverWait` with `expected_conditions`; CSS/XPath selectors; `execute_script()` for scrolling; consistent viewport settings.

Weeks 110â€“112 â€” SQL Tutorial
- ğŸ“– [SQL Tutorial](https://www.sqltutorial.org/)
- ğŸ§ª Practice: Core SELECT/WHERE/JOIN; then subqueries/aggregations; then windows/CTEs (in tutorial order)
- âœ… Pass (weekly): Execute â‰¥20 queries aligned to the weekâ€™s tutorial sections; final week includes a small analytics schema and â‰¥10 window/CTE queries.
- ğŸ› ï¸ How: SQLite/Postgres with seeded sample DB; save each query with expected rowcount.
</details>

ğŸ” Flex â€” ETL mini-project

---------------------------------------------------------------------

<details>
<summary><b>Phase 11 Â· Deep Learning â€” Weeks 113â€“132 (Complete D2L fundamentals, Goodfellow DL)</b></summary>

Weeks 113â€“120 â€” D2L (Fundamentals)
- ğŸ“– [D2L](https://d2l.ai)
- ğŸ§ª Practice: Topic-specific small models exactly as covered (MLP, CNN, RNN; optimization; regularization; data pipelines)
- âœ… Pass (weekly): Train the chapterâ€™s model variant on a toy dataset with fixed seeds and one controlled ablation (optimizer OR regularization) taught in D2L; log curves/metrics.
- ğŸ› ï¸ How: Follow D2Lâ€™s PyTorch/MXNet examples; fix seeds; keep experiments minimal and reproducible.

Week 121 â€” The Illustrated Transformer (Bridge)
- ğŸ“– [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- ğŸ§ª Practice: Understand the Transformer architecture: self-attention mechanism, multi-head attention, positional encoding, encoder-decoder structure.
- âœ… Pass: Implement self-attention from scratch; verify tensor shapes at each step; implement attention masking; write unit tests for: (1) output shape correctness, (2) masked positions get zero attention, (3) attention weights sum to 1.
- ğŸ› ï¸ How: Use NumPy or PyTorch; implement Q, K, V projections; scaled dot-product attention; verify with `assert` statements and test cases.

Weeks 122â€“132 â€” Deep Learning Book (Complete)
- ğŸ“– [Deep Learning Book](https://www.deeplearningbook.org/)
- ğŸ§ª Practice: For each chapter, run a small experiment that demonstrates the chapterâ€™s key concept using building blocks learned in D2L
- âœ… Pass (weekly): Provide a controlled comparison or demonstration plot showing the expected qualitative effect (e.g., different inits, L2 vs dropout, step-size schedules).
- ğŸ› ï¸ How: Small synthetic or standard toy datasets; fixed seeds; log and compare curves cleanly.
</details>

ğŸ” Flex â€” DL recap + tracked mini project

---------------------------------------------------------------------

<details>
<summary><b>Phase 12 Â· MLOps & Data Engineering â€” Weeks 133â€“156 (Complete Zoomcamps, ML Systems)</b></summary>

Weeks 133â€“140 â€” MLOps Zoomcamp
- ğŸ“– [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- ğŸ§ª Practice: Module-by-module implementation as taught (tracking, packaging, CI, serving, orchestration, monitoring)
- âœ… Pass (weekly): A runnable local pipeline from clean state to served endpoint with tests passing for that weekâ€™s scope.
- ğŸ› ï¸ How: Docker/Compose; MLflow/W&B; `pytest`; minimal infra defined as per module.

Weeks 141â€“148 â€” Machine Learning Systems
- ğŸ“– [ML Systems](https://mlsysbook.ai)
- ğŸ§ª Practice: Write/extend a system design doc each week focusing only on that weekâ€™s concepts (SLA/SLOs; rollout/rollback; monitoring; data contracts; cost/reliability)
- âœ… Pass (weekly): The doc includes concrete metrics, failure scenarios, and operational procedures aligned to the chapter.
- ğŸ› ï¸ How: ADR template; simple diagrams-as-code optional (e.g., Mermaid).

Weeks 149â€“156 â€” Data Engineering Zoomcamp
- ğŸ“– [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- ğŸ§ª Practice: Module-by-module pipeline work (ingestion, storage, batch/stream, orchestration, analytics eng, testing) as taught in the course
- âœ… Pass (weekly): Re-deployable pipeline from scratch with idempotent runs for that moduleâ€™s scope.
- ğŸ› ï¸ How: Terraform/Docker where required, dbt, Airflow/Prefect according to the module.
</details>

ğŸ” Flex â€” Ops/engineering consolidation

---------------------------------------------------------------------

<details>
<summary><b>Phase 13 Â· LLMs & Agents â€” Weeks 157â€“160 (Complete HF Course + Agents)</b></summary>

Weeks 157â€“159 â€” Hugging Face Course
- ğŸ“– [HF Course](https://huggingface.co/course/chapter1)
- ğŸ§ª Practice: Learn the Hugging Face ecosystem: load and preprocess datasets, understand tokenizers, fine-tune pretrained models, run inference, evaluate with appropriate metrics.
- âœ… Pass (weekly): Complete the course exercises for that week's chapters; fine-tune a small transformer on a downstream task (e.g., text classification, NER); evaluate with task-appropriate metrics (accuracy, F1, etc.); log all configurations.
- ğŸ› ï¸ How: `transformers` library for models; `datasets` for data loading; `Trainer` API for fine-tuning; `accelerate` for distributed training; Weights & Biases or TensorBoard for logging.

Week 160 â€” HF Agents
- ğŸ“– [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)
- ğŸ§ª Practice: Build AI agents that use tools: understand agent architectures, implement tool calling, handle errors and timeouts, implement safety guardrails.
- âœ… Pass: Build an agent that completes a multi-step task using external tools; implement proper timeout handling; test with an injected failure scenario and verify graceful degradation; document safety checks and limitations.
- ğŸ› ï¸ How: Use Hugging Face agents framework; implement `Tool` classes; set timeouts with `asyncio.timeout` or similar; log all tool calls and responses; implement input validation.
</details>

---------------------------------------------------------------------

<details>
<summary><b>Phase 14 Â· Consolidation, Capstone, Portfolio â€” Weeks 161â€“164</b></summary>

Week 161 â€” statsmodels deep dive
- ğŸ“– [statsmodels](https://www.statsmodels.org/stable/index.html)
- ğŸ§ª Practice: Master statsmodels by reproducing analyses from earlier phases: OLS with diagnostics, GLMs, time series models (ARIMA, VAR), hypothesis testing.
- âœ… Pass: Reproduce two econometric analyses matching original coefficients and standard errors; include full diagnostic suite (heteroskedasticity, autocorrelation tests); apply robust SEs where violations exist.
- ğŸ› ï¸ How: `statsmodels.api.OLS/GLM`; `statsmodels.tsa` for time series; `het_breuschpagan`, `acorr_ljungbox` for diagnostics; `cov_type="HC3"` for robust SEs.

Week 162 â€” scikit-learn deep dive
- ğŸ“– [scikit-learn](https://scikit-learn.org/stable/index.html)
- ğŸ§ª Practice: Create a production-ready ML pipeline template: preprocessing (scaling, encoding), feature selection, model training with CV, hyperparameter tuning, probability calibration.
- âœ… Pass: Build a complete Pipeline with ColumnTransformer for mixed types; implement nested CV for unbiased evaluation; apply probability calibration (Platt scaling or isotonic); ensure deterministic results with fixed seeds.
- ğŸ› ï¸ How: `Pipeline`; `ColumnTransformer`; `GridSearchCV`/`RandomizedSearchCV`; `CalibratedClassifierCV`; fixed `random_state` throughout.

Weeks 163â€“164 â€” Capstone & Portfolio
- ğŸ“– Integrate end-to-end skills only from prior phases
- ğŸ§ª Practice: Complete a capstone project demonstrating: problem framing, data pipeline, modeling with uncertainty quantification, model interpretation, rigorous evaluation, and stakeholder communication.
- âœ… Pass: Deliver a fully reproducible project (single command to run); include README documenting problem, approach, assumptions, limitations, and risks; provide model interpretation (SHAP/PDP); write a 1-page non-technical summary for stakeholders.
- ğŸ› ï¸ How: Use Git for version control; Docker for reproducibility; include uncertainty estimates (bootstrap CIs or Bayesian); create visualizations for non-technical audience; document all decisions.
</details>

---------------------------------------------------------------------

Resource-to-Week Completion Map (cover-to-cover)
- Python for Data Analysis â€” Weeks 1â€“8 â€” [Python for Data Analysis](https://wesmckinney.com/book/)
- Mathematics for Machine Learning â€” Weeks 9â€“18 â€” [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- Think Stats â€” Weeks 19â€“24 â€” [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- Think Bayes â€” Weeks 25â€“32 â€” [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- Flexible Imputation of Missing Data â€” Weeks 33â€“36 â€” [FIMD](https://stefvanbuuren.name/fimd/)
- ISLP (Statistical Learning with Python) â€” Weeks 37â€“46 â€” [ISLP](https://www.statlearning.com/)
- PRML (Bishop) â€” Weeks 47â€“60 â€” [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- Interpretable Machine Learning â€” Weeks 61â€“65 â€” [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- Data Mining: Concepts and Techniques (3e) â€” Weeks 66â€“74 â€” [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- Basic Econometrics (Gujarati) â€” Weeks 75â€“86 â€” [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- New Introduction to Multiple Time Series (LÃ¼tkepohl) â€” Weeks 87â€“96 â€” [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- R for Data Science (2e) â€” Weeks 97â€“106 â€” [R for Data Science (2e)](https://r4ds.hadley.nz)
- Beautiful Soup â€” Week 107 â€” [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- Selenium (Python) â€” Weeks 108â€“109 â€” [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- SQL Tutorial â€” Weeks 110â€“112 â€” [SQL Tutorial](https://www.sqltutorial.org/)
- Dive into Deep Learning â€” Weeks 113â€“120 â€” [D2L](https://d2l.ai)
- The Illustrated Transformer â€” Week 121 â€” [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- Deep Learning â€” Weeks 122â€“132 â€” [Deep Learning Book](https://www.deeplearningbook.org/)
- MLOps Zoomcamp â€” Weeks 133â€“140 â€” [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- Machine Learning Systems â€” Weeks 141â€“148 â€” [ML Systems](https://mlsysbook.ai)
- Data Engineering Zoomcamp â€” Weeks 149â€“156 â€” [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- HF Course + HF Agents â€” Weeks 157â€“160 â€” [HF Course](https://huggingface.co/course/chapter1), [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)

Notes
- Keep work in any format; seed randomness for reproducibility.
- Use Flex Weeks to finish pass items, review tricky parts, and add spaced-repetition cards (optional).
- Back to top â†‘

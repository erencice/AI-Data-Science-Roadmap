# AI & Data Science Weekly Plan â€” Activities, Practice, and Pass Criteria

![Duration](https://img.shields.io/badge/duration-~154_weeks-6f42c1)
![Pace](https://img.shields.io/badge/pace-8â€“10_h%2Fweek-0e8a16)
![Path](https://img.shields.io/badge/path-beginner%E2%86%92practitioner-0366d6)
![Style](https://img.shields.io/badge/style-cumulative%2C_concept%E2%86%92practice-555)

Zero prior knowledge is assumed. Learning order is strictly top-to-bottom. Each week includes a clear â€œPassâ€ requirement aligned to the primary resource.

â€” Quick jump â€”
- Phase 1 Â· Data Analysis Foundations
- Phase 2 Â· Mathematics for ML
- Phase 3 Â· Statistics Fundamentals
- Phase 4 Â· Bayesian Statistics & Missing Data
- Phase 5 Â· Classical ML
- Phase 6 Â· Data Mining
- Phase 7 Â· Econometrics & Time Series
- Phase 8 Â· R for Data Science
- Phase 9 Â· Web Scraping & SQL
- Phase 10 Â· Deep Learning
- Phase 11 Â· MLOps & Data Engineering
- Phase 12 Â· LLMs & Open-Source AI
- Phase 13 Â· Consolidation & Capstone

Legend
- ğŸ“– Activities (primary source)
- ğŸ§ª Practice (small tasks)
- âœ… Pass (weekly pass criterion)
- ğŸ› ï¸ How (implementation hint)
- ğŸ” Flex (catch-up, spaced review)

Duration and pacing
- Duration: ~154 weeks (â‰ˆ3.0 years), 8â€“10 h/week
- Weekly output: small practical tasks only
- Frequent Flex Weeks between phases for consolidation

Main resources (cover-to-cover completion)
- Python for Data Analysis â€” Wes McKinney â€” [Python for Data Analysis](https://wesmckinney.com/book/)
- Mathematics for Machine Learning â€” Deisenroth, Faisal, Ong â€” [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- Think Stats â€” Allen B. Downey â€” [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- Think Bayes â€” Allen B. Downey â€” [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- Flexible Imputation of Missing Data â€” van Buuren â€” [FIMD](https://stefvanbuuren.name/fimd/)
- Pattern Recognition and Machine Learning â€” Bishop â€” [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- Interpretable Machine Learning â€” Molnar â€” [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- Data Mining: Concepts and Techniques (3e) â€” Han, Kamber, Pei â€” [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- Basic Econometrics â€” Gujarati â€” [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- New Introduction to Multiple Time Series â€” LÃ¼tkepohl â€” [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- R for Data Science (2e) â€” Wickham, Ã‡etinkaya-Rundel, Grolemund â€” [R for Data Science (2e)](https://r4ds.hadley.nz)
- Beautiful Soup docs â€” [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- Selenium (Python) docs â€” [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- SQL Tutorial â€” [SQL Tutorial](https://www.sqltutorial.org/)
- Dive into Deep Learning â€” Zhang et al. â€” [D2L](https://d2l.ai)
- Deep Learning â€” Goodfellow, Bengio, Courville â€” [Deep Learning Book](https://www.deeplearningbook.org/)
- MLOps Zoomcamp â€” DataTalksClub â€” [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- Machine Learning Systems â€” Symeonidis et al. â€” [ML Systems](https://mlsysbook.ai)
- Data Engineering Zoomcamp â€” DataTalksClub â€” [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- Hugging Face Course â€” [HF Course](https://huggingface.co/course/chapter1)
- HF Agents Course â€” [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)

Supporting references (selective)
- Trigonometric Cheat Sheet â€” [Trig Sheet (PDF)](https://tutorial.math.lamar.edu/pdf/Trig_Cheat_Sheet.pdf)
- Python Crash Course â€” [Video](https://www.youtube.com/watch?v=rfscVS0vtbw)
- Kevin Sheppard Python Notes â€” [Notes (PDF)](https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2021.pdf)
- PSU STAT â€” [STAT portal](https://online.stat.psu.edu)
- scikit-learn docs â€” [scikit-learn](https://scikit-learn.org/stable/index.html)
- statsmodels docs â€” [statsmodels](https://www.statsmodels.org/stable/index.html)

---------------------------------------------------------------------

<details>
<summary><b>Phase 1 Â· Data Analysis Foundations â€” Weeks 1â€“8 (Complete Python for Data Analysis)</b></summary>

Week 1 â€” P4DA Ch. 1â€“2
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Read CSV; Series/DataFrame basics; indexing; simple plots (exactly from Ch.1â€“2 topics)
- âœ… Pass: One notebook that loads a CSV, uses `.head()/.info()`, selects columns via `.loc/.iloc`, filters rows, and produces 4 labeled matplotlib/seaborn plots.
- ğŸ› ï¸ How: `pd.read_csv`, `.loc/.iloc`, boolean masks, `plot.hist()`, `seaborn.countplot`.

Week 2 â€” P4DA Ch. 3â€“4
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Aggregation with `groupby`, merges/joins, reshaping with `stack/unstack/pivot`
- âœ… Pass: Build a summary table via `groupby().agg()`, merge it to a second table with `pd.merge`, and reshape it with `pivot_table`. Verify row/column counts at each step.
- ğŸ› ï¸ How: `groupby().agg({"col":"mean"})`, `pd.merge(left, right, on="key")`, `pd.pivot_table(values, index, columns, aggfunc)`.

Week 3 â€” P4DA Ch. 5â€“6
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Text cleanup with `.str` methods; date parsing; dtype fixes
- âœ… Pass: Convert a messy date column to `datetime64[ns]`, standardize a string categorical column (trim/lower), and produce a 10-line data dictionary describing each column and dtype.
- ğŸ› ï¸ How: `pd.to_datetime(..., errors="coerce")`, `df["col"].str.strip().str.lower()`, `df.astype`.

Week 4 â€” P4DA Ch. 7â€“8
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Time series indexing; resampling; rolling/window ops (as introduced in Ch.7â€“8)
- âœ… Pass: Set a DateTimeIndex, resample to weekly means, and compute a 7-step rolling mean; plot original vs resampled vs rolling mean in one figure.
- ğŸ› ï¸ How: `df = df.set_index("date")`, `df.resample("W").mean()`, `.rolling(7).mean()`.

Week 5 â€” P4DA Ch. 9â€“10
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Categoricals; pivot tables; tidy summaries
- âœ… Pass: Convert a string column to ordered `Categorical` and produce a pivot table summarizing a numeric metric by that category. Justify the order.
- ğŸ› ï¸ How: `pd.Categorical(df["cat"], categories=[...], ordered=True)`, `pd.pivot_table`.

Week 6 â€” P4DA Ch. 11â€“12 (+appendices)
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Functions and reuse; exporting artifacts; light performance care (vectorization where shown)
- âœ… Pass: Turn your EDA steps into small functions at top of a notebook and parameterize the input filepath; saving 1 CSV and 2 plots. Re-run on a second dataset by changing one variable.
- ğŸ› ï¸ How: Define `load_data(path)`, `clean(df)`, `summarize(df)`; `df.to_csv`, `plt.savefig`.

Week 7 â€” P4DA Project A
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: End-to-end EDA using only chapters 1â€“12 capabilities
- âœ… Pass: Apply your parameterized EDA to a new dataset and write a 1-page memo with â‰¥3 insights, â‰¥2 hypotheses, and â‰¥1 data quality risk.
- ğŸ› ï¸ How: Reuse Week 6 functions; keep code idempotent.

Week 8 â€” P4DA Project B
- ğŸ“– Activities: [Python for Data Analysis](https://wesmckinney.com/book/)
- ğŸ§ª Practice: Feature engineering strictly from transforms covered (dates, ratios, categories)
- âœ… Pass: Create 5 features (date parts, ratios, interactions limited to arithmetic) and document rationale and potential leakage.
- ğŸ› ï¸ How: `df.assign(...)`, `pd.to_datetime(...).dt.month`, arithmetic features.
</details>

ğŸ” Flex â€” Consolidate EDA template and notes

---------------------------------------------------------------------

<details>
<summary><b>Phase 2 Â· Mathematics for ML â€” Weeks 9â€“18 (Complete MML)</b></summary>

Week 9 â€” Linear Algebra I
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Vectors, norms, matrix ops; SVD intro
- âœ… Pass: Compute SVD on a toy matrix and reconstruct it from top-k components; report reconstruction error vs k.
- ğŸ› ï¸ How: `U,S,Vt = np.linalg.svd(A, full_matrices=False)`; `A_k = U[:,:k] @ np.diag(S[:k]) @ Vt[:k]`.

Week 10 â€” Linear Algebra II
- ğŸ“– [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- ğŸ§ª Practice: Eigenvalues/vectors; conditioning
- âœ… Pass: Show eigenvector sensitivity by adding small Gaussian noise to a symmetric matrix and plotting angle change vs noise.
- ğŸ› ï¸ How: `np.linalg.eig`; compute angle via normalized dot product.

Week 11 â€” Decompositions & Geometry
- ğŸ“– [MML Book (PDF)](...)
- ğŸ§ª Practice: QR vs normal equations for least squares
- âœ… Pass: Solve `min ||Ax-b||` via normal equations and via QR; compare residual norms.
- ğŸ› ï¸ How: `np.linalg.qr(A)`; backsolve; `np.linalg.lstsq` for reference.

Week 12 â€” Vector Calculus I
- ğŸ“– [MML Book (PDF)](...)
- ğŸ§ª Practice: Gradients/Jacobians; gradient descent on convex quadratic
- âœ… Pass: Show monotone loss decrease for a suitable step size on `f(x)=1/2 x^T Q x + c^T x`.
- ğŸ› ï¸ How: analytic gradient `Qx+c`; fixed small step.

Week 13 â€” Vector Calculus II
- ğŸ“– [MML Book (PDF)](...)
- ğŸ§ª Practice: Chain rule; finite-difference checks
- âœ… Pass: Compare analytic vs central-difference gradient on a 2D function; max abs diff < 1e-4.
- ğŸ› ï¸ How: central differences with small `h`.

Week 14 â€” Probability I
- ğŸ“– [MML Book (PDF)](...)
- ğŸ§ª Practice: LLN/CLT simulations using distributions covered
- âœ… Pass: For Binomial and Poisson sample means, show variance â‰ˆ theory and QQ-plots trending more linear as n increases.
- ğŸ› ï¸ How: simulate many trials; compute sample mean variance; `scipy.stats.probplot` or manual quantiles.

Week 15 â€” Probability II
- ğŸ“– [MML Book (PDF)](...)
- ğŸ§ª Practice: Covariance; correlation; dependence vs zero-correlation
- âœ… Pass: Generate correlated Normals via Cholesky and recover covariance empirically with small Frobenius error (< 0.05).
- ğŸ› ï¸ How: `L = cholesky(Sigma)`; `X = Z @ L.T`; `np.cov`.

Week 16 â€” Optimization I
- ğŸ“– [MML Book (PDF)](...)
- ğŸ§ª Practice: Convexity via Hessian; backtracking line search
- âœ… Pass: Verify convexity by PSD Hessian for two functions and implement backtracking line search on a convex quadratic.
- ğŸ› ï¸ How: compute Hessian analytically or via finite differences; Armijo condition.

Week 17 â€” Optimization II
- ğŸ“– [MML Book (PDF)](...)
- ğŸ§ª Practice: Compare first- vs second-order methods introduced in MML
- âœ… Pass: Solve ridge-regularized least squares with Gradient Descent (with backtracking) vs Newtonâ€™s method; show iterations-to-tolerance.
- ğŸ› ï¸ How: add Î»I to Q; implement Newton step using Hessian; compare convergence curves.

Week 18 â€” Review
- ğŸ“– [MML Book (PDF)](...)
- ğŸ§ª Practice: Concept map and short-link notes
- âœ… Pass: A one-page map with â‰¥10 links from math concepts to later ML choices (e.g., regularization â†” condition number).
- ğŸ› ï¸ How: diagram or bullet map; keep explicit link statements.
</details>

ğŸ” Flex â€” Retrieval practice and summaries

---------------------------------------------------------------------

<details>
<summary><b>Phase 3 Â· Statistics Fundamentals â€” Weeks 19â€“24 (Complete Think Stats)</b></summary>

Week 19 â€” Think Stats Ch. 1
- ğŸ“– [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- ğŸ§ª Practice: ECDF construction; histogram comparison
- âœ… Pass: Implement ECDF on real data; verify it is non-decreasing and ends at 1.0; compare to histogram shape.
- ğŸ› ï¸ How: `np.sort`; `np.arange(1,n+1)/n`.

Week 20 â€” Think Stats Ch. 2
- ğŸ“– [Think Stats (PDF)](...)
- ğŸ§ª Practice: Robust vs classical descriptive stats
- âœ… Pass: Report mean/SD vs median/MAD/trimmed mean on 2 datasets and explain divergence due to skew/outliers.
- ğŸ› ï¸ How: `scipy.stats.median_abs_deviation`; trimming via slice after sort.

Week 21 â€” Think Stats Ch. 3â€“4
- ğŸ“– [Think Stats (PDF)](...)
- ğŸ§ª Practice: Relationships; Pearson vs Spearman
- âœ… Pass: Show an example where Pearson and Spearman diverge and explain monotone non-linear dependence.
- ğŸ› ï¸ How: `np.corrcoef`; `scipy.stats.spearmanr`.

Week 22 â€” Think Stats Ch. 5â€“6
- ğŸ“– [Think Stats (PDF)](...)
- ğŸ§ª Practice: Basic probability; simple Bayesian update
- âœ… Pass: Compute a Betaâ€“Binomial posterior mean/var analytically and confirm via simulation.
- ğŸ› ï¸ How: closed-form update; simulate posteriors.

Week 23 â€” Think Stats Ch. 7â€“8
- ğŸ“– [Think Stats (PDF)](...)
- ğŸ§ª Practice: Hypothesis testing
- âœ… Pass: Simulate empirical Type I â‰ˆ Î± and produce a power curve for a specified effect size.
- ğŸ› ï¸ How: repeated sampling; count rejections.

Week 24 â€” Think Stats Ch. 9â€“10 (+wrap)
- ğŸ“– [Think Stats (PDF)](...)
- ğŸ§ª Practice: Regression basics; diagnostics
- âœ… Pass: Fit OLS; show residual mean â‰ˆ 0, residual vs fitted plot, and compute VIFs; flag VIF > 10 if any.
- ğŸ› ï¸ How: `statsmodels.api.OLS`; `variance_inflation_factor`.
</details>

ğŸ” Flex â€” Stats recap

---------------------------------------------------------------------

<details>
<summary><b>Phase 4 Â· Bayesian & Missing Data â€” Weeks 25â€“36 (Complete Think Bayes, FIMD)</b></summary>

Weeks 25â€“32 â€” Think Bayes (Ch. 1â€“14, paced)
- ğŸ“– [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- ğŸ§ª Practice: Conjugates; posterior predictive checks; simple model comparison as presented in the book
- âœ… Pass (weekly): Implement a book-aligned Bayesian model (e.g., Betaâ€“Binomial, Gammaâ€“Poisson, Normalâ€“Normal) with prior sensitivity and a posterior predictive check. For comparison, use the approach discussed in the chapter (e.g., predictive performance or simple Bayes factors where applicable).
- ğŸ› ï¸ How: analytic posteriors when available; draw PPC replicates and compare a chosen statistic.

Weeks 33â€“36 â€” Flexible Imputation of Missing Data (complete)
- ğŸ“– [FIMD](https://stefvanbuuren.name/fimd/)
- ğŸ§ª Practice: Missingness mechanisms; MICE; sensitivity (as in book)
- âœ… Pass (weekly): Run MICE (mâ‰¥5) on a dataset; report pooled estimates per Rubinâ€™s rules; compare to complete-case; perform delta-adjustment sensitivity where relevant.
- ğŸ› ï¸ How: use a MICE implementation (e.g., statsmodels/impyute/sklearn-iterative as proxy) consistent with book procedures.
</details>

ğŸ” Flex â€” Consolidate Bayesian + MI

---------------------------------------------------------------------

<details>
<summary><b>Phase 5 Â· Classical ML â€” Weeks 37â€“55 (Complete PRML, Interpretable ML)</b></summary>

Weeks 37â€“50 â€” PRML (Ch. 1â€“13 + review)
- ğŸ“– [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- ğŸ§ª Practice: Implement chapter-aligned core algorithms using only concepts introduced so far (e.g., logistic regression, linear regression with basis functions, naive Bayes, kernels for regression, EM for GMM, simple graphical model inference)
- âœ… Pass (weekly): From-scratch implementation for that chapter demonstrates parity (within 2â€“5%) with a library baseline on a small toy dataset; include seeded reproducibility.
- ğŸ› ï¸ How: use sklearn purely as an oracle for comparison; fix `random_state`; limit to toy-scale experiments.

Weeks 51â€“55 â€” Interpretable ML (complete)
- ğŸ“– [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- ğŸ§ª Practice: Global (PDP/ICE, permutation) and local (e.g., SHAP) methods as presented
- âœ… Pass (weekly): Apply PDP/ICE and permutation importance; then SHAP to the same model; write a 1-page note on stability and limitations across 3 resamples.
- ğŸ› ï¸ How: `sklearn.inspection.partial_dependence/plot_partial_dependence` (or newer API), `permutation_importance`, `shap` for local explanations.
</details>

ğŸ” Flex â€” Validation & interpretation synthesis

---------------------------------------------------------------------

<details>
<summary><b>Phase 6 Â· Data Mining â€” Weeks 56â€“64 (Complete DM 3e)</b></summary>

Weeks 56â€“64 â€” Data Mining 3e (Ch. 1â€“12)
- ğŸ“– [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- ğŸ§ª Practice: Per-chapter algorithmic work strictly matching the chapter (e.g., data preprocessing tasks; Apriori/FP-Growth; decision trees; k-means/DBSCAN; outlier detection)
- âœ… Pass (weekly): Implement a minimal working version for the chapterâ€™s focal algorithm OR replicate results using a library; verify correctness on a deterministic toy and compare performance on a small real dataset.
- ğŸ› ï¸ How: construct small synthetic datasets with known ground truth (fixed seeds); assert counts/clusters/rules match expectation.
</details>

ğŸ” Flex â€” Mining recap

---------------------------------------------------------------------

<details>
<summary><b>Phase 7 Â· Econometrics & Time Series â€” Weeks 65â€“86 (Complete Gujarati, LÃ¼tkepohl)</b></summary>

Weeks 65â€“76 â€” Basic Econometrics (complete)
- ğŸ“– [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- ğŸ§ª Practice: Reproduce a worked example per chapter using methods from that chapter only (OLS basics; classical assumption diagnostics; heteroskedasticity/autocorrelation remedies; functional form; limited dependent variables as presented)
- âœ… Pass (weekly): Match the textbook exampleâ€™s coefficients and standard errors (within rounding) and include one robustness check discussed in that chapter (e.g., robust/HAC SEs when appropriate).
- ğŸ› ï¸ How: `statsmodels` OLS/GLM, `cov_type="HC3"` or HAC if the chapter addresses it; include diagnostic plots taught there.

Weeks 77â€“86 â€” LÃ¼tkepohl (complete)
- ğŸ“– [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- ğŸ§ª Practice: VAR/VECM workflow exactly as the book presents (lag selection, stability checks, IRFs/FEVD, cointegration where applicable)
- âœ… Pass (weekly): Fit VAR/VECM on a macro dataset; pass residual diagnostics; include IRFs/FEVD; perform cointegration tests if required by the chapter.
- ğŸ› ï¸ How: `statsmodels.tsa.api.VAR/VECM`; rolling-origin evaluation for forecast sections.
</details>

ğŸ” Flex â€” Econometrics/time-series consolidation

---------------------------------------------------------------------

<details>
<summary><b>Phase 8 Â· R for Data Science â€” Weeks 87â€“96 (Complete R4DS 2e)</b></summary>

Weeks 87â€“96 â€” R4DS (Complete)
- ğŸ“– [R for Data Science (2e)](https://r4ds.hadley.nz)
- ğŸ§ª Practice: Weekly mini-analyses using only the chapters completed that week (wrangle â†’ visualize â†’ model or summary â†’ render)
- âœ… Pass (weekly): Render a Quarto/Rmd report that re-runs end-to-end with one command, using a seed and only functions introduced in the completed chapters.
- ğŸ› ï¸ How: `tidyverse` verbs for the covered chapters; `set.seed`; optional `targets/drake` for reproducible workflows.
</details>

ğŸ” Flex â€” R consolidation

---------------------------------------------------------------------

<details>
<summary><b>Phase 9 Â· Web Scraping & SQL â€” Weeks 97â€“102 (Complete BeautifulSoup, Selenium, SQL)</b></summary>

Week 97 â€” BeautifulSoup
- ğŸ“– [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- ğŸ§ª Practice: Single static page scrape using only requests + bs4 (selectors, parsing, extraction)
- âœ… Pass: Save structured CSV/JSON with documented schema; respect robots.txt; no 429s.
- ğŸ› ï¸ How: `requests.get`; `BeautifulSoup(html, "lxml")`; `.select` or `.find_all`; `time.sleep` backoff.

Weeks 98â€“99 â€” Selenium
- ğŸ“– [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- ğŸ§ª Practice: Dynamic page automation as per docs (waits, selectors, pagination/scroll)
- âœ… Pass (weekly): Scrape a dynamic page (e.g., infinite scroll or simple login) and save a timestamped, deterministic snapshot with logs of retries/timeouts.
- ğŸ› ï¸ How: `WebDriverWait`; CSS/XPath selectors; consistent viewport and user agent.

Weeks 100â€“102 â€” SQL Tutorial
- ğŸ“– [SQL Tutorial](https://www.sqltutorial.org/)
- ğŸ§ª Practice: Core SELECT/WHERE/JOIN; then subqueries/aggregations; then windows/CTEs (in tutorial order)
- âœ… Pass (weekly): Execute â‰¥20 queries aligned to the weekâ€™s tutorial sections; final week includes a small analytics schema and â‰¥10 window/CTE queries.
- ğŸ› ï¸ How: SQLite/Postgres with seeded sample DB; save each query with expected rowcount.
</details>

ğŸ” Flex â€” ETL mini-project

---------------------------------------------------------------------

<details>
<summary><b>Phase 10 Â· Deep Learning â€” Weeks 103â€“122 (Complete D2L fundamentals, Goodfellow DL)</b></summary>

Weeks 103â€“110 â€” D2L (Fundamentals)
- ğŸ“– [D2L](https://d2l.ai)
- ğŸ§ª Practice: Topic-specific small models exactly as covered (MLP, CNN, RNN; optimization; regularization; data pipelines)
- âœ… Pass (weekly): Train the chapterâ€™s model variant on a toy dataset with fixed seeds and one controlled ablation (optimizer OR regularization) taught in D2L; log curves/metrics.
- ğŸ› ï¸ How: Follow D2Lâ€™s PyTorch/MXNet examples; fix seeds; keep experiments minimal and reproducible.

Week 111 â€” The Illustrated Transformer (Bridge)
- ğŸ“– [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- ğŸ§ª Practice: Self-attention mechanics (shapes, masks, scaling) as explained in the article
- âœ… Pass: Implement toy self-attention and write unit tests for shape/mask/scaling behavior.
- ğŸ› ï¸ How: NumPy/PyTorch; `assert` shape checks; verify mask zeros attention to padded tokens.

Weeks 112â€“122 â€” Deep Learning Book (Complete)
- ğŸ“– [Deep Learning Book](https://www.deeplearningbook.org/)
- ğŸ§ª Practice: For each chapter, run a small experiment that demonstrates the chapterâ€™s key concept using building blocks learned in D2L
- âœ… Pass (weekly): Provide a controlled comparison or demonstration plot showing the expected qualitative effect (e.g., different inits, L2 vs dropout, step-size schedules).
- ğŸ› ï¸ How: Small synthetic or standard toy datasets; fixed seeds; log and compare curves cleanly.
</details>

ğŸ” Flex â€” DL recap + tracked mini project

---------------------------------------------------------------------

<details>
<summary><b>Phase 11 Â· MLOps & Data Engineering â€” Weeks 123â€“146 (Complete Zoomcamps, ML Systems)</b></summary>

Weeks 123â€“130 â€” MLOps Zoomcamp
- ğŸ“– [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- ğŸ§ª Practice: Module-by-module implementation as taught (tracking, packaging, CI, serving, orchestration, monitoring)
- âœ… Pass (weekly): A runnable local pipeline from clean state to served endpoint with tests passing for that weekâ€™s scope.
- ğŸ› ï¸ How: Docker/Compose; MLflow/W&B; `pytest`; minimal infra defined as per module.

Weeks 131â€“138 â€” Machine Learning Systems
- ğŸ“– [ML Systems](https://mlsysbook.ai)
- ğŸ§ª Practice: Write/extend a system design doc each week focusing only on that weekâ€™s concepts (SLA/SLOs; rollout/rollback; monitoring; data contracts; cost/reliability)
- âœ… Pass (weekly): The doc includes concrete metrics, failure scenarios, and operational procedures aligned to the chapter.
- ğŸ› ï¸ How: ADR template; simple diagrams-as-code optional (e.g., Mermaid).

Weeks 139â€“146 â€” Data Engineering Zoomcamp
- ğŸ“– [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- ğŸ§ª Practice: Module-by-module pipeline work (ingestion, storage, batch/stream, orchestration, analytics eng, testing) as taught in the course
- âœ… Pass (weekly): Re-deployable pipeline from scratch with idempotent runs for that moduleâ€™s scope.
- ğŸ› ï¸ How: Terraform/Docker where required, dbt, Airflow/Prefect according to the module.
</details>

ğŸ” Flex â€” Ops/engineering consolidation

---------------------------------------------------------------------

<details>
<summary><b>Phase 12 Â· LLMs & Agents â€” Weeks 147â€“150 (Complete HF Course + Agents)</b></summary>

Weeks 147â€“149 â€” Hugging Face Course
- ğŸ“– [HF Course](https://huggingface.co/course/chapter1)
- ğŸ§ª Practice: Datasets, tokenizers, training, inference as covered by the course units
- âœ… Pass (weekly): Fine-tune or run inference with a small transformer; evaluate with a suitable metric; log configs exactly as the course demonstrates.
- ğŸ› ï¸ How: `transformers`, `datasets`, `accelerate`; keep to course-recommended scripts.

Week 150 â€” HF Agents
- ğŸ“– [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)
- ğŸ§ª Practice: Tool-using agent with timeouts and error handling as per course
- âœ… Pass: Agent completes a simple multi-step task within timeouts and handles one injected failure path gracefully; list safety checks.
- ğŸ› ï¸ How: Use course framework; implement guards/timeouts as shown.
</details>

---------------------------------------------------------------------

<details>
<summary><b>Phase 13 Â· Consolidation, Capstone, Portfolio â€” Weeks 151â€“154</b></summary>

Week 151 â€” statsmodels deep dive
- ğŸ“– [statsmodels](https://www.statsmodels.org/stable/index.html)
- ğŸ§ª Practice: Reproduce two econometric analyses from earlier phases using only covered methods
- âœ… Pass: Match reference coefficients/SEs within tolerance; include robust SEs where applicable.

Week 152 â€” scikit-learn deep dive
- ğŸ“– [scikit-learn](https://scikit-learn.org/stable/index.html)
- ğŸ§ª Practice: Build a clean template ML pipeline using methods you have already learned (preprocess â†’ CV â†’ metric â†’ calibration if relevant)
- âœ… Pass: Deterministically re-runs and produces calibrated probabilities (if classification).

Weeks 153â€“154 â€” Capstone & Portfolio
- ğŸ“– Integrate end-to-end skills only from prior phases
- ğŸ§ª Practice: Capstone with uncertainty quantification, interpretability, evaluation protocol, and non-technical brief
- âœ… Pass: Reproducible project script; README with assumptions/risks; clear results and decisions.
</details>

---------------------------------------------------------------------

Resource-to-Week Completion Map (cover-to-cover)
- Python for Data Analysis â€” Weeks 1â€“8 â€” [Python for Data Analysis](https://wesmckinney.com/book/)
- Mathematics for Machine Learning â€” Weeks 9â€“18 â€” [MML Book (PDF)](https://course.ccs.neu.edu/ds4420sp20/readings/mml-book.pdf)
- Think Stats â€” Weeks 19â€“24 â€” [Think Stats (PDF)](https://greenteapress.com/thinkstats/thinkstats.pdf)
- Think Bayes â€” Weeks 25â€“32 â€” [Think Bayes](https://open.umn.edu/opentextbooks/textbooks/think-bayes-bayesian-statistics-made-simple)
- Flexible Imputation of Missing Data â€” Weeks 33â€“36 â€” [FIMD](https://stefvanbuuren.name/fimd/)
- PRML (Bishop) â€” Weeks 37â€“50 â€” [PRML (PDF)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- Interpretable Machine Learning â€” Weeks 51â€“55 â€” [Interpretable ML](https://christophm.github.io/interpretable-ml-book/)
- Data Mining: Concepts and Techniques (3e) â€” Weeks 56â€“64 â€” [Data Mining 3e (PDF)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf)
- Basic Econometrics (Gujarati) â€” Weeks 65â€“76 â€” [Gujarati (PDF)](https://www.cbpbu.ac.in/userfiles/file/2020/STUDY_MAT/ECO/1.pdf)
- New Introduction to Multiple Time Series (LÃ¼tkepohl) â€” Weeks 77â€“86 â€” [LÃ¼tkepohl (PDF)](https://www.cur.ac.rw/mis/main/library/documents/book_file/2005_Book_NewIntroductionToMultipleTimeS.pdf)
- R for Data Science (2e) â€” Weeks 87â€“96 â€” [R for Data Science (2e)](https://r4ds.hadley.nz)
- Beautiful Soup â€” Week 97 â€” [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- Selenium (Python) â€” Weeks 98â€“99 â€” [Selenium (Python)](https://selenium-python.readthedocs.io/index.html)
- SQL Tutorial â€” Weeks 100â€“102 â€” [SQL Tutorial](https://www.sqltutorial.org/)
- Dive into Deep Learning â€” Weeks 103â€“110 â€” [D2L](https://d2l.ai)
- The Illustrated Transformer â€” Week 111 â€” [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- Deep Learning â€” Weeks 112â€“122 â€” [Deep Learning Book](https://www.deeplearningbook.org/)
- MLOps Zoomcamp â€” Weeks 123â€“130 â€” [MLOps Zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)
- Machine Learning Systems â€” Weeks 131â€“138 â€” [ML Systems](https://mlsysbook.ai)
- Data Engineering Zoomcamp â€” Weeks 139â€“146 â€” [DE Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- HF Course + HF Agents â€” Weeks 147â€“150 â€” [HF Course](https://huggingface.co/course/chapter1), [HF Agents](https://huggingface.co/learn/agents-course/unit0/introduction)

Notes
- Keep work in any format; seed randomness for reproducibility.
- Use Flex Weeks to finish pass items, review tricky parts, and add spaced-repetition cards (optional).
- Back to top â†‘
